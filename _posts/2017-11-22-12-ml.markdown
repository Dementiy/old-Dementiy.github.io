---
layout: post
title: Прогнозируем, классифицируем и кластеризуем (draft)
categories: python golang R datascience практики
---

Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.

Перед выполнением каждого задания дается краткое описание рассматриваемой задачи (с минимальными математическими выкладками), приводится пример возможного решения на языке R, далее следует само задание, а именно реализация описываемых алгоритмов на языке Python (подразумевается использование модулей `numpy` и `pandas` для выполнения векторных операций) и в конце приводится пример решения задания с использованием библиотеки `sklearn`.

<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"><img src="/assets/images/11-ml/sklearn_algos.png" width="100%"></a>

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### Регрессия

**Линейная регрессия** – метод восстановления зависимости между двумя переменными:

$$y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$$

Задача заключается в поиске такого набора параметров $$\theta$$, чтобы получать как можно лучшие результаты в предсказании $$y$$. В этом задании мы будем рассмотривать метод градиентного спуска для подбора параметров $$\theta$$ (существуют и другие методы, например, метод наименьших квадратов).

В качестве датасета нам послужат данные собранные в 1991 году Л. Уиллерманом (Willerman Lee) о 40 правополушарных студентах, изучавших курс психологии в Юго-Западном университете. Исследование, проведенное Уиллерманом, заключалось в поиске связи между размером мозга и коэффициентом интеллекта. Для исследования выбирались те студенты, которые не страдали алкоголизмом, потерей сознания, а также повреждениями мозга, эпилепсией или сердечной недостаточностью. Для определения размера мозга испытуемых исследователи использовали магнитно-резонансную томографию. Также каждому студенту было предложено пройти тесты Векслера. Информация о поле, весе и росте также была включена в данные. Таким образом, собранные Уиллерманом данные включают в себя следующие переменные:

 - пол (Gender) – мужской или женский;
 - значение комбинированного полного коэффициента интеллекта (FSIQ – Full Scale IQ);
 - коэффициент вербального интеллекта (VIQ – Verbal IQ);
 - баллы, набранные по тестам Векслера (PIQ – Performance IQ);
 - вес тела в фунтах (Weight);
 - рост в дюймах (Height);
 - количество пикселей на снимках МРТ (MRI_Count).

В качестве простого примера построим двумерную модель для определения (прогнозирования) коэффициента вербального интеллекта VIQ по коэффициенту комбинированного полного интеллекта FSIQ.

Загрузим набор данных из репозитория и построим диаграмму рассеяния для соответствующих переменных:

```r
> library(repmis)
> df <- source_data("https://raw.githubusercontent.com/Dementiy/pybook-assignments/master/homework11/brain_size.csv")
> head(df)
  Gender FSIQ VIQ PIQ Weight Height MRI_Count
1 Female  133 132 124    118   64.5    816932
2   Male  140 150 124     NA   72.5   1001121
3   Male  139 123 150    143   73.3   1038437
4   Male  133 129 128    172   68.8    965353
5 Female  137 132 134    147   65.0    951545
6 Female   99  90 110    146   69.0    928799
> X <- df$FSIQ # input variables
> y <- df$VIQ  # output variables
> plot(X, y,
    main='Full Scale IQ vs Verbal IQ',
    xlab="FSIQ", ylab="VIQ")
```

<img src="/assets/images/11-ml/linreg1_new.png" width="70%">

### *Метод градиентного спуска*

Метод градиентного спуска это простой метод для поиска локального минимума функции. Подбор параметров $$\theta$$ происходит в соответствии со следующим правилом:

$$\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)$$

Где $$J(\theta)$$ называется целевой функцией (cost function), а $$\alpha$$ скоростью обучения (learning rate). Целевая функция вычисляется по следующей формуле:

$$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2 \rightarrow \frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

В результате подстановки получим следующее правило для пересчета параметров $$\theta$$:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Более подробно про метод градиентного спуска можно почитать <a href="http://mccormickml.com/2014/03/04/gradient-descent-derivation/">тут</a>.</p>
</div>

$$\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

В репозитории уже есть готовая реализация метода градиентного спуска на языке R, поэтому давайте воспользуемся ей:

```r
> source("https://raw.githubusercontent.com/Dementiy/pybook-assignments/master/homework11/gradient_descent.R")
> result <- gradientDescent(X, y, 0.00001, 100)
> result$theta
            [, 1]
[1, ] 0.008634596
[2, ] 0.987622436
> plot(X, y,
    main='Full Scale IQ vs Verbal IQ',
    xlab="FSIQ", ylab="VIQ")
> abline(a=result$theta[1], b=result$theta[2])
```

<img src="/assets/images/11-ml/linreg3.png" width="70%">

Таким образом, итоговая модель для предсказания значения вербального интеллекта будет выглядеть следующим образом:

$$VIQ = 0.008634596 + 0.987622436 \times FSIQ$$

Например, в случае, когда комбинированный полный коэффициент интеллекта принимает значение в 100 баллов, то значение вербального интеллекта будет равно 98.77 баллам:

```r
> 0.008634596 + 0.987622436 * 100
[1] 98.77088
```

Построим графики сходимости целевой функции:

```r
> plot(result$cost_history, type='l', col='blue', lwd=2, 
    main='Cost function', ylab='cost', xlab='Iterations')
```

<img src="/assets/images/11-ml/linreg2.png" width="70%">

Следует заметить, что приведенная реализация метода градиентного спуска основывается на постоянном шаге $$\alpha$$, также мы положили число итераций равное 100, но на графике сходимости целевой функции хорошо видно, что мы можем уменьшить требуемое число итераций.

### *Задание*

Вашей задачей является реализовать метод градиентного спуска:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Имена методов выбраны в соответствии с тем как они названы в <code>sklearn</code>.</p>
</div>

```python
class GDRegression:
    
    def __init__(self, alpha=0.01, n_iter=100):
        pass

    def fit(self, X_train, y_train):
        pass

    def predict(self, X_test):
        pass
```

Метод `fit` обучает модель на данных `X_train` (матрица признаков) и `y_train`(вектор ответов). Результатом работы этого метода являются два атрибута: `coef_` - вектор оценок для $$\theta_i$$ ($$i$$ принимает значения от 1 до $$p$$, где $$p$$ - это количество признаков) и `intercept_` - оцененное значение для $$\theta_0$$.

Метод `predict` возвращает вектор прогнозов для новых данных.

Давайте рассмотрим пример использования класса `GDRegression`. Для начала загрузим набор данных, выведем пять первых наблюдений и построим диаграмму рассеивания:

```python
>>> import pandas as pd
>>> import matplotlib.pyplot as plt
>>> data = pd.read_csv("brain_size.csv")
>>> data.head()
   Gender  FSIQ  VIQ  PIQ  Weight  Height  MRI_Count
0  Female   133  132  124   118.0    64.5     816932
1    Male   140  150  124     NaN    72.5    1001121
2    Male   139  123  150   143.0    73.3    1038437
3    Male   133  129  128   172.0    68.8     965353
4  Female   137  132  134   147.0    65.0     951545
>>> plt.scatter(data["FSIQ"], data["VIQ"])
>>> plt.show()
```

Теперь создадим модель с параметрами `alpha=0.00001` и `n_iter=100` и обучим ее на всех имеющихся данных:

```python
>>> X = data.iloc[:, 1:2] # to keep it as DataFrame
>>> # X.insert(0, "Ones", np.ones(len(X)))
>>> y = data["VIQ"]
>>> model = GDRegression(alpha=0.00001)
>>> model.fit(X, y)
>>> model.coef_
array([ 0.98762244])
>>> model.intercept_
0.0086345960432628564
```

Итак, мы получили следующие оценки для параметров $$\theta$$: $$\theta_0 = 0.00863459$$, $$\theta_1 = 0.98762244$$. Не сложно заметить, что они совпадают с предыдущими значениями.

Давайте сделаем несколько прогнозов для имеющихся данных:

```python
>>> model.predict(X.iloc[1:5, :])
array([138.275366, 137.287766, 131.362163, 135.312565])
```

### *Пример с использованием библиотеки sklearn*

```python
>>> from sklearn.linear_model import SGDRegression
>>> data = pd.read_csv("brain_size.csv")
>>> X = data.iloc[:, 1:2]
>>> y = data["VIQ"]
>>> model = SGDRegression(n_iter=5, penalty='none',
    learning_rate='constant', eta0=0.00001)
>>> model.fit(X, y)
>>> model.coef_
array([ 0.97827634])
>>> model.intercept_
array([ 0.00892904])
>>> model.predict(X.iloc[1:5, :])
array([ 136.96761663,  135.98934029,  130.11968225,  134.03278761])
```

### *Выбор модели*

```python
def rmse(y_hat, y):
    errors = y_hat - y
    total_error = np.dot(errors, errors)
    return np.sqrt(total_error/len(y_hat))
```


```python
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.3, random_state=0)
>>> X_train.shape, y_train.shape, X_test.shape, y_test.shape
((28, 1), (12, 1), (28,), (12,))
>>> model = GDRegression(alpha=0.00001)
>>> model.fit(X_train, y_train)
>>> model.coef_
array([ 0.98536805])
>>> model.intercept_
array([ 0.00886374])
>>> pred = model.predict(X_test)
>>> rmse(pred, y_test)
6.7600543165795841
```

```python
>>> from sklearn.model_selection import KFold
>>> kf = KFold(n_splits=3)
>>> errors = []
>>> for train, test in kf.split(X):
       X_train = X.iloc[train, :]
       y_train = y[train]
       model.fit(X_train, y_train)
       pred = model.predict(X_test)
       errors.append(rmse(pred, y_test))
>>> print(errors)
[10.3443, 7.1500, 6.5616]
```

**Задания**:
1. Используя кросс-валидацию построить лучшую модель.
2. Реализовать метод стохастического градиентного спуска.

### Классификация

Вы уже встречались с задачей классфикации в работе [«Персонализация новостной ленты Hacker News»](http://127.0.0.1:4000/2017/11/22/06-hackernews/). В общем виде задачу классификации можно представить следующим образом: имеется множество объектов, которые разделены на классы по некоторым признакам. Например, успевающие студенты и отстающие студенты. В обучающей выборке задано конечное множество объектов и их признаков. Например, перечень всех студентов учебного заведения и все оценки по прошедшим и текущим дисциплинам. Для каждого из объектов обучающей выборки известно, к каким классам они относятся. Принадлежность же остальных объектов к классам неизвестна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества, то есть указать наименование (или номер) класса, к которому объект отнесён в результате применения алгоритма классификации.

### *Логистическая регрессия*

Давайте рассмотрим логистическую регрессию на примере набора данных цветов ириса.

Для простоты будем использовать только два признака `Sepal Width` и `Sepal Length`, а также два класса `Setosa` и `Versicolor`:

```python
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
%matplotlib inline
```

```python
data = datasets.load_iris()
X = data.data[:100, :2]
y = data.target[:100]

setosa = plt.scatter(X[:50,0], X[:50,1], c='b')
versicolor = plt.scatter(X[50:,0], X[50:,1], c='r')
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.legend((setosa, versicolor), ("Setosa", "Versicolor"))
sns.despine()
```

Итак, наша задача построить разделяющую границу (decision boundary), которая бы позволила нам отделить наблюдения одного класса (`Setosa`) от другого (`Versicolor`). Одним из напрашиваемых решений является представление зависимой переменной в виде линейной комбинации признаков (по аналогии с линейной регрессией):

$$y_i = \theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i}$$

Предположим, что у нас есть наблюдение с признаками $$SW = 3.5$$ и $$SL = 5$$. И кто-то сказал нам, что $$\theta_0 = 1$$, $$\theta_1 = 2$$ и $$\theta_2 = 4$$, тогда:

$$y_i = 1 + 2 * 3.5 + 4 * 5 = 28$$

Во-первых, как мы должны интерпретировать значене $$28$$? Во-вторых, модель в таком виде подходит для прогнозирования непрерывных значений, которые ограничены бесконечностью:

$$-\infty \le \theta^{T}x \le +\infty$$

В действительности мы хотим построить модель, которая позволит нам прогнозировать бинарный отклик, а точнее вероятность отнесения наблюдения к одному из классов, то есть:

$$P(y=1 \mid x) = f(\theta^{T}x)$$

$$P(y=0 \mid x) = 1 - f(\theta^{T}x)$$

Где:

$$0 \le f(\theta^{T}x) \le 1$$

Такой функцией $$f$$ может быть **логистическая функция**:

$$P(y = 1 \mid x) = \frac{e^{\theta^{T}x}}{1 + e^{\theta^{T}x}} = \frac{1}{1 + e^{-\theta^{T}x}}$$

```python
x_values = np.linspace(-5, 5, 100)
y_values = [1 / (1 + math.e**(-x)) for x in x_values]
plt.plot(x_values, y_values)
plt.axhline(.5)
plt.axvline(0)
sns.despine()
```

Отметим, что значение вероятности, описываемое логистической функцией, ограничено диапазоном значений от 0 до 1, а также то, что большие изменения вероятности требуют больших изменений в $$x$$, чем для значений вероятности близких к 0.5.

Возвращаясь к нашему примеру:

$$P(y = 1 \mid x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i})}} = \frac{1}{1 + e^{-28}} = 0.99$$

Таким образом, с вероятностью 0.99 мы можем отнести наше наблюдение к классу `Setosa`.

Как мы будем находить параметры $$\theta_i$$? Для начала нам необходимо определиться с видом целевой функции. Так как наша модель прогнозирует вероятности, то мы можем использовать принцип максимального правдоподобия.

Для краткости, можем записать функцию распределения $$y$$ при заданном $$x$$ в следующем виде:

$$P(y = y_i \mid x) = P(y_i = 1 \mid x_i)^{y_{i}} (1 - P(y_i = 1 \mid x_i))^{1 - y_{i}}$$

Фактически, это есть распределение Бернулли.

В зависимости от значения $$y$$ одна из двух частей уравнения будет равна 1. 

Для $$y=1$$:

$$P(y = 1 \mid x) = P(y_i = 1 \mid x_i)^1 (1 - P(y_i = 1 \mid x_i))^{1 - 1} = P(y_i = 1 \mid x_i)$$

Для $$y=0$$:

$$P(y = 0 \mid x) = P(y_i = 1 \mid x_i)^0 (1 - P(y_i = 1 \mid x_i))^{1 - 0} = 1 - P(y_i = 1 \mid x_i)$$

Правдоподобие для $$m$$ наблюдений можно записать как:

$$L = \prod_{i=1}^{m}P(y = y_i \mid x_i) = \prod_{i=1}^{m}P(y = 1 \mid x_i)^{y_i}(1 - P(y_i = 1 \mid x_i))^{1 - y_i}$$

$$\log L = \sum_{i=1}^{m} y_i \times \log(P(y = 1 \mid x_i)) + (1 - y_i) \times \log(1 - P(y_i = 1 \mid x_i))$$

Введем обозначение:

$$p_i = P(y = 1 \mid x_i)$$

Тогда функция правдоподобия может быть записана как:

$$\log L = \sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))$$

Таким образом, мы можем записать нашу целевую функцию:

$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))$$

Чтобы найти параметры $$\theta$$ мы можем воспользоваться методом градиентного спуска. Вспомним правило обновления весов:

$$\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)$$

Где $$\frac{\partial}{\partial \theta}J(\theta)$$:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Вывод производных см. <a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/">тут</a>.</p>
</div>

$$\frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$

$$h_{\theta}(x) = f(\theta^Tx) = \frac{1}{1 + e^{-\theta^{T}x}}$$

$$ f(z) = \frac{1}{1 + e ^{-z}}$$

В результате подстановки получим следующее правило для пересчета параметров $$\theta$$:

$$\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

### *Пример с использованием библиотеки sklearn*

```python
from sklearn import linear_model
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

model = linear_model.LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
sum(y_test == y_pred) # 30
```

```python
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred) # 1
```

Давайте рассмотрим другой пример, для которого сгенеируем набор данных из 10 тысяч наблюдений:

```python
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=5, random_state=17)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

model = linear_model.LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

```python
>>> sum(y_test == y_pred)
2306

>>> accuracy_score(y_test, y_pred)
0.76866666666666672
```

Значение $$0.7687$$ ничего не говорит нам об ошибках [первого и второго рода](https://ru.wikipedia.org/wiki/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B8_%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B3%D0%BE_%D0%B8_%D0%B2%D1%82%D0%BE%D1%80%D0%BE%D0%B3%D0%BE_%D1%80%D0%BE%D0%B4%D0%B0). Мы можем построить матрицу ошибок $$C$$, чтобы получить более полное представление.

```python
>>> from sklearn.metrics import confusion_matrix
>>> confusion_matrix(y_test, y_pred)
array([[1129,  363],
       [ 331, 1177]])
```

где:
- $$C_{0,0} = 1129$$ – True Negatives (TN), истинное значение 0, предсказанное 0
- $$C_{1,0} = 331$$ – False Negatives (FN), истинное значение 1, предсказанное 0
- $$C_{0,1} = 363$$ – False Positives (FP), истинное значение 0, предсказанное 1
- $$C_{1,1} = 1177$$ – True Positives (TP), истинное значение 1, предсказанное 1

И наконец мы можем построить ROC-кривую для визуальной оценки точности работы классификатора:

```python
from sklearn.metrics import roc_curve, auc

def plot_roc_curve(y_pred, y_test):
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

y_pred = model.predict_proba(X_test)[:,1]
plot_roc_curve(y_pred, y_test)
```

### *Многоклассовая классификация*

```python
>>> data = datasets.load_iris()
>>> X = data.data
>>> y = data.target

>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

>>> model = linear_model.LogisticRegression(multi_class='ovr')
>>> model.fit(X_train, y_train)

>>> y_pred = model.predict(X_test)
>>> sum(y_test == y_pred)
42

>>> accuracy_score(y_test, y_pred)
0.93333333333333335

>>> confusion_matrix(y_test, y_pred)
array([[12,  0,  0],
       [ 0, 16,  3],
       [ 0,  0, 14]])
```

```python
def plot_roc_curve(y_predict_proba, y_test, n_classes=3):
    # Compute ROC curve and ROC AUC for each class
    n_classes = 3
    fpr = {}
    tpr = {}
    roc_auc = {}
    all_y_test_i = np.array([])
    all_y_predict_proba = np.array([])

    for i in range(n_classes):
        y_test_i = [1 if yi == i else 0 for yi in y_test]
        all_y_test_i = np.concatenate([all_y_test_i, y_test_i])
        all_y_predict_proba = np.concatenate([all_y_predict_proba, y_predict_proba[:, i]])
        fpr[i], tpr[i], _ = roc_curve(y_test_i, y_predict_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Compute micro-average ROC curve and ROC area
    fpr["average"], tpr["average"], _ = roc_curve(all_y_test_i, all_y_predict_proba)
    roc_auc["average"] = auc(fpr["average"], tpr["average"])

    # Plot average ROC Curve
    plt.figure()
    plt.plot(fpr["average"], tpr["average"],
        label='Average ROC curve (area = {0:0.2f})'.format(roc_auc["average"]),
        color='deeppink', linestyle=':', linewidth=4)

    # Plot each individual ROC curve
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Some extension of Receiver operating characteristic to multi-class')
    plt.legend(loc="lower right")
    plt.show()
```

### *Задание*

```python
class LogisticRegression:
    
    def __init__(self, alpha=0.01, max_iter=1000):
        pass

    def fit(self, X_test, y_test):
        pass

    def predict(self, X_train):
        pass

    def predict_proba(self, X_train):
        pass
```

### Кластеризация

**Кластерный анализ** – это способ группировки многомерных объектов, основанный на представлении результатов отдельных наблюдений точками подходящего геометрического пространства с последующим выделением групп как «сгустков» этих точек (кластеров, таксонов). Задачей такого разделения на группы является ухватить естественную структуру данных и абстрагироваться от индивидуальных характеристик каждого объекта к более общим признакам, которые объединяют эти объекты в кластеры. Например, кластеризация документов по их содержимому или кластеризация покупателей по их потребительской корзине и т.д. Так как заранее не известно по каким признакам следует объединять объекты в кластеры, то кластерный анализ относят к методам **обучения без учителя** (unsupervised learning).

Одним из наиболее простых и распространенных алгоритмов кластеризации является алгоритм k-средних (k-means), в котором каждый кластер представлен его центром (центроидом).

 - $$k$$ означает число кластеров. Число кластеров не определяется автоматически и в каждом кластере может быть разное число объектов;
 - k-средних использует двух шаговый эвристический подход к группированию похожих объектов: **шаг присваивания** и **шаг обновления**;
 - схожесть (похожесть) объектов измеряется с помощью **функции дистанции**.

### *Вычисление дистанции между объектами*
 - в методе k-средних значение каждого признака объекта воспринимается как координата в многомерном пространстве (например, если у нам известен только рост и вес человека, то мы имеем дело с двумерным объектом, где одна из координат это рост, а вторая - вес);
 - схожесть объектов можно вычислить используя любую математическую функцию (метрику) схожести;
 - обычно выбирается Евклидово расстояние:
	$$dist(x,y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$$

### *Алгоритм k-средних*

k-средних можно описать следующими 4-мя шагами:
1. Выбрать k объектов как начальные центроиды.
2. Отнести остальные объекты к ближайшим центроидам.
3. Произвести перерасчет центроидов.
4. Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут «двигаться».

Пример поиска трех кластеров, используя алгоритм K-средних представлен на рисунках ниже:

![](/assets/images/11-ml/kmeans1.png)![](/assets/images/11-ml/kmeans2.png)
![](/assets/images/11-ml/kmeans3.png)![](/assets/images/11-ml/kmeans4.png)
![](/assets/images/11-ml/kmeans5.png)![](/assets/images/11-ml/kmeans6.png)
![](/assets/images/11-ml/kmeans7.png)![](/assets/images/11-ml/kmeans8.png)

### *Замечания по k-средних*
* k-средних эвристический алгоритм и не является детерминированным:
    - начальное положение центроидов оказывает существенное влияние на конечный результат;
    - резуьтаты могут быть разными даже на одном и том же наборе данных.
* так как используется расстояние для измерения схожести объектов, то все данные должны быть числовыми:
    - числовые данные должны быть приведены к единой шкале;
    - могут возникнуть проблемы при кластеризации наборов данных состоящих из категориальных переменных.
* иногда требуется знать предметную область и работать по методу проб и ошибок, чтобы получить нужные кластеры:
    - нужно заранее знать число кластеров `k`;
    - может потребоваться оценка эксперта для ответа на вопрос *Являются ли полученные кластеры значимыми?*

### *Как выбирать начальные положения центроидов?*
 - можем случайным образом генерировать центры кластеров;
 - или случайно выбрать k-объектов и назначить их центрами;
 - но существуют и другие алгоритмы, которые позволяют увеличить точность и скорость сходимости k-средних, например:
> Baswade, Nalwade. Selection of Initial Centroids for k-Means Algorithm
>
> 1. From n objects calculate a point whose attribute values are average of n-objects attribute values. So first initial centroid is average on n-objects.
> 2. Select next initial centroids from n-objects in such a way that the Euclidean distance of that object is maximum from other selected initial centroids.
> 3. Repeat step 2 until we get k initial centroids. From these steps we will get initial centroids and with these initial centroids perform k-Means algorithm.

### *Подготовка данных к кластеризации*

Так как в методе k-средних значение каждого признака является координатой в многомерном пространстве, то все значения должны быть числовыми. Кроме того, хорошей практикой является приведение всех значений к единой шкале:
 - min-max нормализация:
 $$x_{new} = \frac{x-min(x)}{max(x)-min(x)}$$
 - z-score:
 $$x_{new} = \frac{x - \mu}{\sigma} = \frac{x - mean(x)}{stddev(x)}$$
 - введение порога:
 $$x_{new} = 1 \mbox{ if } x \ge c \mbox{ else } 0$$

Пример функций нормализации данных на языке R:
```r
# create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))
}

# create z-score standartization function (exists in R, @see ?scale)
scale <- function(x) {
  return (x - mean(x)) / sd(x)
}

# create threshold function
threshold <- function(x, c) {
  return (ifelse(x > c, 1, 0)
}
```

Вашей задачей является написать класс `KMeans` со следующим интерфейсом:

```python
class KMeans:
    
    def __init__(self, n_clusters):
        pass

    def fit(self, X):
        pass

    def predict(self, y):
        pass
```

Для обучения модели будем использовать хорошо известный набор данных с лепестками цветов ириса:

```python
df = pd.read_csv('iris.csv')
X = df.loc[:, data.columns != 'Name'].as_matrix()
model = KMeans(3)
model.fit(X)
clusters = models._clusters
```

```python
plt.figure(figsize=(14,7))

# Create a colormap
colormap = np.array(['red', 'lime', 'black'])

# Plot Sepal
plt.subplot(1, 2, 1)
plt.scatter(df.SepalLength, df.SepalWidth, c=colormap[clusters], s=40)
plt.title('Sepal')

plt.subplot(1, 2, 2)
plt.scatter(df.PetalLength, df.PetalWidth, c=colormap[clusters], s=40)
plt.title('Petal')
```

### *Как выбирать число кластеров?*

Одной из проблем кластеризации является верное определение числа кластеров. Одним из используемых методов, для решения этой проблемы, является отображение зависимости изменения суммы квадратов ошибок от выбранного числа кластеров:

```r
mydata <- bd.rates
wss <- (nrow(mydata) - 1) * sum(apply(mydata), 2, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(mydata, centers = i)$withinss)
plot(1:15, wss, type='b', 
           xlab='Number of clusters', 
           ylab='Within groups sum of squares')
```

