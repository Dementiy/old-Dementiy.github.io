---
layout: post
title: Прогнозируем, классифицируем и кластеризуем
categories: python golang R datascience практики
---

Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.

Перед выполнением каждого задания дается краткое описание рассматриваемой задачи (с минимальными математическими выкладками), приводится пример возможного решения на языке R, далее следует само задание, а именно реализация описываемых алгоритмов на языке Python (подразумевается использование модулей `numpy` и `pandas` для выполнения векторных операций) и в конце приводится пример решения задания с использованием библиотеки `sklearn`.

<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"><img src="/assets/images/11-ml/sklearn_algos.png" width="100%"></a>

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

### Регрессия

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Подробнее про линейную регрессию можно почитать в <a href="">«Заметки по линейной регрессии»</a>.</p>
</div>

Линейная регрессия это метод восстановления зависимости между двумя переменными:

$$y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$$

Задача заключается в поиске такого набора параметров $$\theta$$, чтобы получать как можно лучшие результаты в предсказании $$y$$. В этом задании мы будем рассмотривать метод градиентного спуска для подбора параметров $$\theta$$ (существуют и другие методы, например, метод наименьших квадратов).

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Описание набора данных взято из курса <a href="http://www.uic.unn.ru/~zny/ml/Others/ml_pop.pdf">Н.Ю.Золотых</a>.</p>
</div>

В качестве набора данных мы будем использовать [The Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).  База содержит информацию о
загородных домах близ Бостона, собранную службой переписи населения США. Данные были собраны в 1970-х годах. Информация агрегирована: территория поделена на участки и дома, стоящие на одном участке, собраны в группы. Нужно оценить среднюю цену дома. Таким образом, объектами являются сами эти группы. Их общее количество — 506. В качестве признаков рассматриваются:

- `CRIM` — уровень преступности на душу населения;
- `ZN` — процент земли, застроенной жилыми домами (только для участков площадью свыше 25000 кв. - футов);
- `INDUS` — процент деловой застройки;
- `CHAS` — 1, если участок граничит с рекой; 0 в противном случае (бинарный признак);
- `NOX` — концентрация оксида азота, деленная на 10^7;
- `RM` — среднее число комнат (по всем домам рассматриваемого участка);
- `AGE` — процент домов, построенных до 1940 г. и занимаемых владельцами;
- `DIS` — взвешенное расстояние до 5 деловых центров Бостона;
- `RAD` — индекс удаленности до радиальных магистралей;
- `TAX` — величина налога в $10000;
- `PTRATIO` — количество учащихся, приходящихся на одного учителя (по городу);
- `B = 1000(AA − 0.63)^2`, где `AA` — доля афро-американцев;
- `LSTAT` — процент жителей с низким социальным статусом.

Признак `CHAS` — бинарный, остальные — количественные. Выходом является переменная `MEDV`, равная медианному значению цены строения (по всем домам участка) в $1000.

Для начала загрузим набор данных и выведем пять первых наблюдений:

```python
import pandas as pd
from sklearn.datasets import load_boston

boston = load_boston()
data = pd.DataFrame(data=boston.data, columns=boston.feature_names)
data['MEDV'] = boston.target
data.head()
```

```
      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \
0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   
1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   
2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   
3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   
4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   

   PTRATIO       B  LSTAT  MEDV  
0     15.3  396.90   4.98  24.0  
1     17.8  396.90   9.14  21.6  
2     17.8  392.83   4.03  34.7  
3     18.7  394.63   2.94  33.4  
4     18.7  396.90   5.33  36.2 
```

Мы не будем уделять время разведночному анализу данных, но для желающих поближе познакомиться с набором данных можно начать с этой [статьи](http://www.neural.cz/dataset-exploration-boston-house-pricing.html).

### *Простая линейная регрессия*

Мы начнем с построения простой линейной модели с одной переменной. Переменную выберем по значению коэффициента корреляции Пирсона с целевым признаком `MEDV`:

```python
pearson = data.corr(method='pearson')
pearson["MEDV"].drop("MEDV").sort_values(ascending=False)
```

```
RM         0.695360
ZN         0.360445
B          0.333461
DIS        0.249929
CHAS       0.175260
AGE       -0.376955
RAD       -0.381626
CRIM      -0.385832
NOX       -0.427321
TAX       -0.468536
INDUS     -0.483725
PTRATIO   -0.507787
LSTAT     -0.737663
Name: MEDV, dtype: float64
```

<div class="admonition note">
  <p class="first admonition-title"><strong>Вопрос</strong></p>
  <p class="last">Объясните почему значение коэффициента корреляции между <code>MEDV</code> и переменными <code>RM</code> и <code>LSTAT</code> имеет противоположные знаки (исходя из описания переменных).</p>
</div>

Наиболее высокий коэффициент корреляции у двух переменных: `RM` (среднее число комнат) и `LSTAT` (процент жителей с низким социальным статусом). Выберем переменную `RM` для объяснения `MEDV`.

Давайте построим диаграмму рассеяния и визуально оценим взаимосвязь между средним числом комнат на участке и средней ценой дома на этом участке:

```python
sns.jointplot(data["RM"], data["MEDV"], kind='scatter', joint_kws={'alpha':0.5});
```

<img src="/assets/images/11-ml/lr_jointplot.png">

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про аномалии и выбросы можно почитать <a href="https://alexanderdyakonov.wordpress.com/2017/04/19/%D0%BF%D0%BE%D0%B8%D1%81%D0%BA-%D0%B0%D0%BD%D0%BE%D0%BC%D0%B0%D0%BB%D0%B8%D0%B9-anomaly-detection/">блоге Александра Дьяконова</a>.</p>
</div>
Из диаграммы рассеяния хорошо видно, что имеет место положительная линейная связь между переменными `RM` и `MEDV`, то есть дома с большим числом комнат имеют более высокую цену. Также можем сделать следующие предположения:
- 16 наблюдений `MEDV` имеют значение 50.0. Скорее всего эти наблюдения содержали пропуски или не было возможности указать действительную цену дома, поэтому эти наблюдения могут быть исключены из набора данных.
- одно наблюдение признака `RM` имеет значение 8.78. Это наблюдение может рассматриваться как выброс и может быть исключено из набора данных;
- два наблюдение признака `RM` имеют значения меньше 4. Наблюдения можно рассматривать как выбросы и должны быть исключены из набора данных.

Итак, модель которую мы будем строить имеет следующий вид:

$$MEDV = \theta_0 + \theta_1 \times RM$$

Нам необходимо найти оптимальные значения для параметров $$\theta$$.

### *Метод градиентного спуска*

Метод градиентного спуска это простой метод для поиска локального минимума функции. Подбор параметров $$\theta$$ происходит в соответствии со следующим правилом:

$$\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)$$

Где $$J(\theta)$$ называется целевой функцией (cost function), а $$\alpha$$ скоростью обучения (learning rate). Целевая функция вычисляется по следующей формуле:

$$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2 \rightarrow \frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

В результате подстановки получим следующее правило для пересчета параметров $$\theta$$:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Более подробно про метод градиентного спуска можно почитать <a href="http://mccormickml.com/2014/03/04/gradient-descent-derivation/">тут</a>.</p>
</div>

$$\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

Вашей задачей является реализовать метод градиентного спуска:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Имена методов выбраны в соответствии с тем как они названы в <code>sklearn</code>.</p>
</div>

```python
class GDRegressor:
    
    def __init__(self, alpha=0.01, max_iter=100):
        pass

    def fit(self, X_train, y_train):
        pass

    def predict(self, X_test):
        pass
```

Метод `fit` обучает модель на данных `X_train` (матрица признаков) и `y_train`(вектор ответов). Результатом работы этого метода являются два атрибута: `coef_` - вектор оценок для $$\theta_i$$ ($$i$$ принимает значения от 1 до $$p$$, где $$p$$ - это количество признаков) и `intercept_` - оцененное значение для $$\theta_0$$.

Метод `predict` возвращает вектор прогнозов для новых данных.

Давайте рассмотрим пример использования класса `GDRegressor`. Предварительно разобьем выборку на обучающую и тестовую и обучим модель на обучающей выборке, положив значения для параметров `max_iter=2000` и `alpha=0.04`.

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про кросс-валидацию с примерами можно почитать <a href="https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6">тут</a>.<br><br>У вас значения <code>model.coef_</code> и <code>model.intercept_</code> могут немного отличаться от полученных мной.</p>
</div>

<div class="admonition note">
  <p class="first admonition-title"><strong>Вопрос</strong></p>
  <p class="last">Дайте интерпретацию полученным значениям для <code>model.coef_</code> и <code>model.intercept_</code>.</p>
</div>

```python
>>> X = data[["RM"]]
>>> y = data["MEDV"]

>>> X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=18)
>>> model = GDRegressor(max_iter=2000, alpha=0.04)
>>> model.fit(X_train, Y_train)

>>> model.coef_, model.intercept_
(array([7.07554766]), -21.792447302876575)

>>> data.plot(kind='scatter', x="RM", y="MEDV");
>>> plt.plot(X_train, model.coef_[0] * X_train + model.intercept_, 'r');
```

<img src="/assets/images/11-ml/lr_fitted1.png">

### *Оценка качества модели*

Итак, мы нашли коэффициенты $$\theta$$, теперь надо как-то оценить качество работы полученной модели. Для этого мы воспользуемся коэффициентом детерминации $$R^2$$:

$$R^2 = 1 - \frac{\sum_{i=1}^{m}(y_i - \hat{y_i})^2}{\sum_{i=1}^{m}(y_i - \overline{y})^2}$$

и среднеквадратичной ошибкой:

$$RMSE = \sqrt{\frac{\sum_{i=1}^{m}(\hat{y_i} - y_i)^2}{m}}$$

[Коэффициент детерминации](http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8) это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью. Более точно — это единица минус доля необъяснённой дисперсии (дисперсии случайной ошибки модели, или условной по признакам дисперсии зависимой переменной) в дисперсии зависимой переменной.

Среднеквадратичная ошибка характеризует отклонение реальных данных от линии регрессии и измеряется в тех же единицах, что и зависимая переменная ($$y$$).

Вашей задачей является написать две функции для вычисления коэфиициента детерминации и среднеквадратичной ошибки:

```python
def rmse(y_hat, y):
    """ Root mean squared error """
    pass

def r_squared(y_hat, y):
    """ R-squared score """
    pass
```

```python
>>> Y_pred = model.predict(X_test)
>>> rmse(Y_pred, Y_test)
6.500256917031142
>>> r_squared(Y_pred, Y_test)
0.488226294688234
```

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про анализ регрессионных остатков можно почитать <a href="http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/">тут</a> и <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D1%85_%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%BE%D0%B2_(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80)">тут</a>.</p>
</div>

Построим график истинных значений и прогнозов:

```python
plt.scatter(Y_test, Y_pred)
plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")
plt.title("Prices vs Predicted prices: $Y_i$ vs $\hat{Y}_i$");
```

<img src="/assets/images/11-ml/lr_predicted1.png">

Также построим график остатков (residual plot):

```python
plt.scatter(Y_pred, (Y_pred-Y_test), alpha=0.5);
```

<img src="/assets/images/11-ml/lr_residuals1.png">

Можем ли мы улучшить качество нашей модели? Мы выбрали значения для параметров `max_iter` и `alpha` «случайным» образом. Воспользуйтесь функцией `plot_cost_function` для того, чтобы найти оптимальные значения для числа итераций `max_iter` и параметра `alpha`. Например:

```python
plot_cost_function(X, y, GDRegressor, max_iters=2000, alpha=0.04)
```

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Обратите внимание, что строится график зависимости MSE (RMSE^2) от номера текущей итерации.</p>
</div>

<img src="/assets/images/11-ml/lr_costfunc1.png">

Очевидно, что на двух тысячах итераций и скорости обучения $$0.04$$ алгоритм еще не сошелся. Попробуйте увеличить число итераций или изменить значение `alpha` (или и то и другое). В итоге вы должны получить значение для среднеквадратичной ошибки $$6.433$$, а для коэффициента детерминации $$0.498$$.

В начале мы сделали несколько предположений о выбросах, давайте проверим одно из них. Удалите из набора данных наблюдения, значения которых для признака `MEDV` равны 50. И снова воспользуйтесь функцией `plot_cost_function` для определения числа итераций и скорости обучения. Также ответье на вопрос «Улучшилось ли качество модели?».

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про важность нормализации признаков можно почитать <a href="https://www.robertoreif.com/blog/2017/12/16/importance-of-feature-scaling-in-data-modeling-part-1-h8nla">тут</a> и <a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#z-score-standardization-or-min-max-scaling">тут</a>.</p>
</div>

Вы должны были обратить внимание, что увеличилось необходимое число итераций для сходимости алгоритма. Одним из способов повлиять на это является нормализация признаков. Мы будем использовать один из самых простых и распространенных способов нормализации - Standart Scaling (Z-score normalization):

$$x' = \frac{x - \overline{x}}{\sigma_x}$$

Напишите функцию `z_scaler` для нормализации значений:

```python
def z_scaler(feature):
    pass

>>> X_scaled = z_scaler(X)
>>> y_scaled = z_scaler(y)
```

Снова воспользуйтесь функцией `plot_cost_function`, но начните с небольшого числа итераций и при необходимости постепенно его увеличивайте.

### *Пример с использованием библиотеки sklearn*

Проделайте теже шаги с использованием библиотеки `sklearn`: с помощью функции `plot_cost_function` с параметром `normalize=True` найдите оптимальное число итераций и значение для скорости обучения, вычислите значение среднеквадратичной ошибки и коэффициента детерминации, постройте графики остатков.

Для того, чтобы построить модель:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Обратите внимание, что скорость обучение задается с помощью параметра <code>eta0</code>.</p>
</div>

```python
>>> from sklearn.linear_model import SGDRegressor
>>> model = SGDRegressor(
        loss='squared_loss',
        learning_rate='constant',
        max_iter=max_iter,
        eta0=eta0)
```

Чтобы нормализовать значения для обучения и валидации модели воспользуйтесь `StandardScaler` из библиотеки `sklearn`:

```python
>>> from sklearn.preprocessing import StandardScaler
>>> X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.33, random_state = 18)
>>> X_train = StandardScaler().fit_transform(X_train)
>>> Y_train = StandardScaler().fit_transform(Y_train.values.reshape(-1, 1))
>>> X_test = StandardScaler().fit_transform(X_test)
>>> Y_test = StandardScaler().fit_transform(Y_test.values.reshape(-1, 1))
```

Чтобы найти среднеквадратичную ошибку и коэффициент детерминации:

```python
>>> from sklearn.metrics import r2_score
>>> r2_score(Y_test, Y_pred)

>>> from sklearn.metrics import mean_squared_error
>>> mean_squared_error(Y_test, Y_pred)
```

В результате вы должны получить следующие значения:
 - коэффициенты модели - $$\theta_0 = 5.85827955e-05, \theta_1 = 0.64210931$$;
 - коэффициент детерминации - $$0.561$$;
 - среднеквадратичная ошибка - $$0.439$$.

### *Многомерная линейная регрессия*

До этого мы рассматривали только один признак - среднее число комнат на участке. Давайте в нашу модель добавим еще один признак - процент жителей с низким социальным статусом (`LSTAT`).

```python
sns.jointplot(data["LSTAT"], data["MEDV"], kind='scatter', joint_kws={'alpha':0.5});
```

<img src="/assets/images/11-ml/lr_jointplot2.png">

Снова воспользуемся функцией `plot_cost_function`:

```python
X_filtered = data[(data["MEDV"] < 50)][["RM", "LSTAT"]]
y_filtered = data[(data["MEDV"] < 50)]["MEDV"]

plot_cost_function(
    X_filtered,
    y_filtered,
    estimator=SGDRegressor,
    normalize=True,
    max_iters=???, # PUT YOUR VALUE HERE
    eta0=???       # PUT YOUR VALUE HERE
)
```

Постройте и обучите модель для найденных значений `max_iter` и `alpha`. Вы должны получить следующие значения для среднеквадратичной ошибки и коэффициента детерминации:

```python
>>> Y_pred = model.predict(X_test)
>>> mean_squared_error(Y_test, Y_pred), r2_score(Y_test, Y_pred)
(0.29456340651451396, 0.7054365934854863)
```

Добавление нового признака значительно снизило среднеквадратичную ошибку и повысило описательную способность нашей модели. Давайте построим график остатков:

```python
plt.scatter(Y_pred, (Y_pred - Y_test.ravel()), alpha=0.5);
```

<img src="/assets/images/11-ml/lr_residuals2.png">

Из графика хорошо видно, что есть нелинейная зависимость в остатках. Это связано с тем, что процент жителей с низким социальным статусом нелинейно зависит от средней цены на дом и среднего числа комнат: 

<img src="/assets/images/11-ml/lr_scatter3d.png">

Эту зависимость наша модель в настоящий момент не описывает. Таким образом, мы должны добавить в нашу модель нелинейные признаки.

### *Добавление полиномиальных признаков*

Итак, для описания нелинейной зависимости мы будем использовать полиномиальную модель. Предположим, что полинома четвертой степени достаточно для описания зависимости. Возможно будет достаточно полинома второй или третьей степени, поэтому, чтобы «занулить» некоторые коэффициенты $$\theta$$, мы будем использовать L1-регуляризацию.

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(4, include_bias=False)
X = poly.fit_transform(X_filtered)
```

```python
plot_cost_function(
    X_filtered,
    y_filtered,
    SGDRegressor,
    normalize=True,
    max_iters=???, # PUT YOUR VALUE HERE
    eta0=???,      # PUT YOUR VALUE HERE
    params={
        'penalty': 'l1',
        'alpha': ??? # PUT YOUR VALUE HERE
    }
)
```

Постройте и обучите модель для найденных значений `max_iter` и `alpha`. Вы должны получить примерно следующие значения для среднеквадратичной ошибки и коэффициента детерминации:

```python
Y_pred = model.predict(X_test)
mean_squared_error(Y_test, Y_pred), r2_score(Y_test, Y_pred)
(0.17508905829300977, 0.8249109417069903)
```

И снова мы значительно снизили значение среднеквадратичной ошибки и повысили описательную способность нашей модели. Давайте посмотрим на график остатков:

```python
plt.scatter(Y_pred, (Y_pred-Y_test.reshape(1, -1)), alpha=0.5);
```

<img src="/assets/images/11-ml/lr_residuals3.png">

Также давайте посмотрим на полученные значения коэффициентов модели:

```python
model.coef_, model.intercept_
#        RM           LSTAT        RM^2         RM*LSTAT     LSTAT^2
(array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#        RM^3         RM^2*LSTAT   RM*LSTAT^2   LSTAT^3      RM^4
         0.62890582,  0.        ,  0.        ,  0.        ,  0.        ,
#        RM^3*LSTAT   RM^2*LSTAT^2 RM*LSTAT^3   LSTAT^4
        -0.52099058,  0.        ,  0.        ,  0.0317591 ]),
 array([0.0018161]))
```

Итак, если мы подставим значения ненулевых коэффициентов, то получим следующиую модель:

$$
\text{MEDV} = 0.0018161 + 0.62890582 \times \text{RM}^3 - 0.52099058 \times \text{RM}^3 \times \text{LSTAT} + 0.0317591 \times \text{LSTAT}^4
$$

Модель стала немного сложнее, появился также эффект взаимодействия между признаками `RM` и `LSTAT`.

Определите какие еще признаки имеет смысл включить в модель.

### Классификация

Вы уже встречались с задачей классфикации в работе [«Персонализация новостной ленты Hacker News»](http://127.0.0.1:4000/2017/11/22/06-hackernews/). В общем виде задачу классификации можно представить следующим образом: имеется множество объектов, которые разделены на классы по некоторым признакам. Например, успевающие студенты и отстающие студенты. В обучающей выборке задано конечное множество объектов и их признаков. Например, перечень всех студентов учебного заведения и все оценки по прошедшим и текущим дисциплинам. Для каждого из объектов обучающей выборки известно, к каким классам они относятся. Принадлежность же остальных объектов к классам неизвестна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества, то есть указать наименование (или номер) класса, к которому объект отнесён в результате применения алгоритма классификации.

### *Логистическая регрессия*

Давайте рассмотрим логистическую регрессию на примере набора данных цветов ириса.

Для простоты будем использовать только два признака `Sepal Width` и `Sepal Length`, а также два класса `Setosa` и `Versicolor`:

```python
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
%matplotlib inline
```

```python
data = datasets.load_iris()
X = data.data[:100, :2]
y = data.target[:100]

setosa = plt.scatter(X[:50,0], X[:50,1], c='b')
versicolor = plt.scatter(X[50:,0], X[50:,1], c='r')
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.legend((setosa, versicolor), ("Setosa", "Versicolor"))
sns.despine()
```

Итак, наша задача построить разделяющую границу (decision boundary), которая бы позволила нам отделить наблюдения одного класса (`Setosa`) от другого (`Versicolor`). Одним из напрашиваемых решений является представление зависимой переменной в виде линейной комбинации признаков (по аналогии с линейной регрессией):

$$y_i = \theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i}$$

Предположим, что у нас есть наблюдение с признаками $$SW = 3.5$$ и $$SL = 5$$. И кто-то сказал нам, что $$\theta_0 = 1$$, $$\theta_1 = 2$$ и $$\theta_2 = 4$$, тогда:

$$y_i = 1 + 2 * 3.5 + 4 * 5 = 28$$

Во-первых, как мы должны интерпретировать значене $$28$$? Во-вторых, модель в таком виде подходит для прогнозирования непрерывных значений, которые ограничены бесконечностью:

$$-\infty \le \theta^{T}x \le +\infty$$

В действительности мы хотим построить модель, которая позволит нам прогнозировать бинарный отклик, а точнее вероятность отнесения наблюдения к одному из классов, то есть:

$$P(y=1 \mid x) = f(\theta^{T}x)$$

$$P(y=0 \mid x) = 1 - f(\theta^{T}x)$$

Где:

$$0 \le f(\theta^{T}x) \le 1$$

Такой функцией $$f$$ может быть **логистическая функция**:

$$P(y = 1 \mid x) = \frac{e^{\theta^{T}x}}{1 + e^{\theta^{T}x}} = \frac{1}{1 + e^{-\theta^{T}x}}$$

```python
x_values = np.linspace(-5, 5, 100)
y_values = [1 / (1 + math.e**(-x)) for x in x_values]
plt.plot(x_values, y_values)
plt.axhline(.5)
plt.axvline(0)
sns.despine()
```

Отметим, что значение вероятности, описываемое логистической функцией, ограничено диапазоном значений от 0 до 1, а также то, что большие изменения вероятности требуют больших изменений в $$x$$, чем для значений вероятности близких к 0.5.

Возвращаясь к нашему примеру:

$$P(y = 1 \mid x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i})}} = \frac{1}{1 + e^{-28}} = 0.99$$

Таким образом, с вероятностью 0.99 мы можем отнести наше наблюдение к классу `Setosa`.

Как мы будем находить параметры $$\theta_i$$? Для начала нам необходимо определиться с видом целевой функции. Так как наша модель прогнозирует вероятности, то мы можем использовать принцип максимального правдоподобия.

Для краткости, можем записать функцию распределения $$y$$ при заданном $$x$$ в следующем виде:

$$P(y = y_i \mid x) = P(y_i = 1 \mid x_i)^{y_{i}} (1 - P(y_i = 1 \mid x_i))^{1 - y_{i}}$$

Фактически, это есть распределение Бернулли.

В зависимости от значения $$y$$ одна из двух частей уравнения будет равна 1. 

Для $$y=1$$:

$$P(y = 1 \mid x) = P(y_i = 1 \mid x_i)^1 (1 - P(y_i = 1 \mid x_i))^{1 - 1} = P(y_i = 1 \mid x_i)$$

Для $$y=0$$:

$$P(y = 0 \mid x) = P(y_i = 1 \mid x_i)^0 (1 - P(y_i = 1 \mid x_i))^{1 - 0} = 1 - P(y_i = 1 \mid x_i)$$

Правдоподобие для $$m$$ наблюдений можно записать как:

$$L = \prod_{i=1}^{m}P(y = y_i \mid x_i) = \prod_{i=1}^{m}P(y = 1 \mid x_i)^{y_i}(1 - P(y_i = 1 \mid x_i))^{1 - y_i}$$

$$\log L = \sum_{i=1}^{m} y_i \times \log(P(y = 1 \mid x_i)) + (1 - y_i) \times \log(1 - P(y_i = 1 \mid x_i))$$

Введем обозначение:

$$p_i = P(y = 1 \mid x_i)$$

Тогда функция правдоподобия может быть записана как:

$$\log L = \sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))$$

Таким образом, мы можем записать нашу целевую функцию:

$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))$$

Чтобы найти параметры $$\theta$$ мы можем воспользоваться методом градиентного спуска. Вспомним правило обновления весов:

$$\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)$$

Где $$\frac{\partial}{\partial \theta}J(\theta)$$:

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Вывод производных см. <a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/">тут</a>.</p>
</div>

$$\frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$

$$h_{\theta}(x) = f(\theta^Tx) = \frac{1}{1 + e^{-\theta^{T}x}}$$

$$ f(z) = \frac{1}{1 + e ^{-z}}$$

В результате подстановки получим следующее правило для пересчета параметров $$\theta$$:

$$\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

### *Пример с использованием библиотеки sklearn*

```python
from sklearn import linear_model
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

model = linear_model.LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
sum(y_test == y_pred) # 30
```

```python
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred) # 1
```

Давайте рассмотрим другой пример, для которого сгенеируем набор данных из 10 тысяч наблюдений:

```python
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=5, random_state=17)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

model = linear_model.LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

```python
>>> sum(y_test == y_pred)
2306

>>> accuracy_score(y_test, y_pred)
0.76866666666666672
```

Значение $$0.7687$$ ничего не говорит нам об ошибках [первого и второго рода](https://ru.wikipedia.org/wiki/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B8_%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B3%D0%BE_%D0%B8_%D0%B2%D1%82%D0%BE%D1%80%D0%BE%D0%B3%D0%BE_%D1%80%D0%BE%D0%B4%D0%B0). Мы можем построить матрицу ошибок $$C$$, чтобы получить более полное представление.

```python
>>> from sklearn.metrics import confusion_matrix
>>> confusion_matrix(y_test, y_pred)
array([[1129,  363],
       [ 331, 1177]])
```

где:
- $$C_{0,0} = 1129$$ – True Negatives (TN), истинное значение 0, предсказанное 0
- $$C_{1,0} = 331$$ – False Negatives (FN), истинное значение 1, предсказанное 0
- $$C_{0,1} = 363$$ – False Positives (FP), истинное значение 0, предсказанное 1
- $$C_{1,1} = 1177$$ – True Positives (TP), истинное значение 1, предсказанное 1

И наконец мы можем построить ROC-кривую для визуальной оценки точности работы классификатора:

```python
from sklearn.metrics import roc_curve, auc

def plot_roc_curve(y_pred, y_test):
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

y_pred = model.predict_proba(X_test)[:,1]
plot_roc_curve(y_pred, y_test)
```

### *Многоклассовая классификация*

```python
>>> data = datasets.load_iris()
>>> X = data.data
>>> y = data.target

>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

>>> model = linear_model.LogisticRegression(multi_class='ovr')
>>> model.fit(X_train, y_train)

>>> y_pred = model.predict(X_test)
>>> sum(y_test == y_pred)
42

>>> accuracy_score(y_test, y_pred)
0.93333333333333335

>>> confusion_matrix(y_test, y_pred)
array([[12,  0,  0],
       [ 0, 16,  3],
       [ 0,  0, 14]])
```

```python
def plot_roc_curve(y_predict_proba, y_test, n_classes=3):
    # Compute ROC curve and ROC AUC for each class
    n_classes = 3
    fpr = {}
    tpr = {}
    roc_auc = {}
    all_y_test_i = np.array([])
    all_y_predict_proba = np.array([])

    for i in range(n_classes):
        y_test_i = [1 if yi == i else 0 for yi in y_test]
        all_y_test_i = np.concatenate([all_y_test_i, y_test_i])
        all_y_predict_proba = np.concatenate([all_y_predict_proba, y_predict_proba[:, i]])
        fpr[i], tpr[i], _ = roc_curve(y_test_i, y_predict_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Compute micro-average ROC curve and ROC area
    fpr["average"], tpr["average"], _ = roc_curve(all_y_test_i, all_y_predict_proba)
    roc_auc["average"] = auc(fpr["average"], tpr["average"])

    # Plot average ROC Curve
    plt.figure()
    plt.plot(fpr["average"], tpr["average"],
        label='Average ROC curve (area = {0:0.2f})'.format(roc_auc["average"]),
        color='deeppink', linestyle=':', linewidth=4)

    # Plot each individual ROC curve
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Some extension of Receiver operating characteristic to multi-class')
    plt.legend(loc="lower right")
    plt.show()
```

### *Задание*

```python
class LogisticRegression:
    
    def __init__(self, alpha=0.01, max_iter=1000):
        pass

    def fit(self, X_test, y_test):
        pass

    def predict(self, X_train):
        pass

    def predict_proba(self, X_train):
        pass
```

### Кластеризация

**Кластерный анализ** – это способ группировки многомерных объектов, основанный на представлении результатов отдельных наблюдений точками подходящего геометрического пространства с последующим выделением групп как «сгустков» этих точек (кластеров, таксонов). Задачей такого разделения на группы является ухватить естественную структуру данных и абстрагироваться от индивидуальных характеристик каждого объекта к более общим признакам, которые объединяют эти объекты в кластеры. Например, кластеризация документов по их содержимому или кластеризация покупателей по их потребительской корзине и т.д. Так как заранее не известно по каким признакам следует объединять объекты в кластеры, то кластерный анализ относят к методам **обучения без учителя** (unsupervised learning).

Одним из наиболее простых и распространенных алгоритмов кластеризации является алгоритм k-средних (k-means), в котором каждый кластер представлен его центром (центроидом).

 - $$k$$ означает число кластеров. Число кластеров не определяется автоматически и в каждом кластере может быть разное число объектов;
 - k-средних использует двух шаговый эвристический подход к группированию похожих объектов: **шаг присваивания** и **шаг обновления**;
 - схожесть (похожесть) объектов измеряется с помощью **функции дистанции**.

### *Вычисление дистанции между объектами*
 - в методе k-средних значение каждого признака объекта воспринимается как координата в многомерном пространстве (например, если у нам известен только рост и вес человека, то мы имеем дело с двумерным объектом, где одна из координат это рост, а вторая - вес);
 - схожесть объектов можно вычислить используя любую математическую функцию (метрику) схожести;
 - обычно выбирается Евклидово расстояние:
	$$dist(x,y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$$

### *Алгоритм k-средних*

k-средних можно описать следующими 4-мя шагами:
1. Выбрать k объектов как начальные центроиды.
2. Отнести остальные объекты к ближайшим центроидам.
3. Произвести перерасчет центроидов.
4. Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут «двигаться».

Пример поиска трех кластеров, используя алгоритм K-средних представлен на рисунках ниже:

![](/assets/images/11-ml/kmeans1.png)![](/assets/images/11-ml/kmeans2.png)
![](/assets/images/11-ml/kmeans3.png)![](/assets/images/11-ml/kmeans4.png)
![](/assets/images/11-ml/kmeans5.png)![](/assets/images/11-ml/kmeans6.png)
![](/assets/images/11-ml/kmeans7.png)![](/assets/images/11-ml/kmeans8.png)

### *Замечания по k-средних*
* k-средних эвристический алгоритм и не является детерминированным:
    - начальное положение центроидов оказывает существенное влияние на конечный результат;
    - резуьтаты могут быть разными даже на одном и том же наборе данных.
* так как используется расстояние для измерения схожести объектов, то все данные должны быть числовыми:
    - числовые данные должны быть приведены к единой шкале;
    - могут возникнуть проблемы при кластеризации наборов данных состоящих из категориальных переменных.
* иногда требуется знать предметную область и работать по методу проб и ошибок, чтобы получить нужные кластеры:
    - нужно заранее знать число кластеров `k`;
    - может потребоваться оценка эксперта для ответа на вопрос *Являются ли полученные кластеры значимыми?*

### *Как выбирать начальные положения центроидов?*
 - можем случайным образом генерировать центры кластеров;
 - или случайно выбрать k-объектов и назначить их центрами;
 - но существуют и другие алгоритмы, которые позволяют увеличить точность и скорость сходимости k-средних, например:
> Baswade, Nalwade. Selection of Initial Centroids for k-Means Algorithm
>
> 1. From n objects calculate a point whose attribute values are average of n-objects attribute values. So first initial centroid is average on n-objects.
> 2. Select next initial centroids from n-objects in such a way that the Euclidean distance of that object is maximum from other selected initial centroids.
> 3. Repeat step 2 until we get k initial centroids. From these steps we will get initial centroids and with these initial centroids perform k-Means algorithm.

### *Подготовка данных к кластеризации*

Так как в методе k-средних значение каждого признака является координатой в многомерном пространстве, то все значения должны быть числовыми. Кроме того, хорошей практикой является приведение всех значений к единой шкале:
 - min-max нормализация:
 $$x_{new} = \frac{x-min(x)}{max(x)-min(x)}$$
 - z-score:
 $$x_{new} = \frac{x - \mu}{\sigma} = \frac{x - mean(x)}{stddev(x)}$$
 - введение порога:
 $$x_{new} = 1 \mbox{ if } x \ge c \mbox{ else } 0$$

Пример функций нормализации данных на языке R:
```r
# create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))
}

# create z-score standartization function (exists in R, @see ?scale)
scale <- function(x) {
  return (x - mean(x)) / sd(x)
}

# create threshold function
threshold <- function(x, c) {
  return (ifelse(x > c, 1, 0)
}
```

Вашей задачей является написать класс `KMeans` со следующим интерфейсом:

```python
class KMeans:
    
    def __init__(self, n_clusters):
        pass

    def fit(self, X):
        pass

    def predict(self, y):
        pass
```

Для обучения модели будем использовать хорошо известный набор данных с лепестками цветов ириса:

```python
df = pd.read_csv('iris.csv')
X = df.loc[:, data.columns != 'Name'].as_matrix()
model = KMeans(3)
model.fit(X)
clusters = models._clusters
```

```python
plt.figure(figsize=(14,7))

# Create a colormap
colormap = np.array(['red', 'lime', 'black'])

# Plot Sepal
plt.subplot(1, 2, 1)
plt.scatter(df.SepalLength, df.SepalWidth, c=colormap[clusters], s=40)
plt.title('Sepal')

plt.subplot(1, 2, 2)
plt.scatter(df.PetalLength, df.PetalWidth, c=colormap[clusters], s=40)
plt.title('Petal')
```

### *Как выбирать число кластеров?*

Одной из проблем кластеризации является верное определение числа кластеров. Одним из используемых методов, для решения этой проблемы, является отображение зависимости изменения суммы квадратов ошибок от выбранного числа кластеров:

```r
mydata <- bd.rates
wss <- (nrow(mydata) - 1) * sum(apply(mydata), 2, var))
for (i in 2:15)
    wss[i] <- sum(kmeans(mydata, centers = i)$withinss)
plot(1:15, wss, type='b', 
           xlab='Number of clusters', 
           ylab='Within groups sum of squares')
```

