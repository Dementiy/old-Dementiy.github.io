---
layout: post
title: Прогнозируем, классифицируем и кластеризуем (draft)
categories: python golang R datascience практики
---

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.

Перед выполнением каждого задания дается краткое описание рассматриваемой задачи (с минимальными математическими выкладками), приводится пример возможного решения на языке R, далее следует само задание, а именно реализация описываемых алгоритмов на языке Python (подразумевается использование модулей `numpy` и `pandas` для выполнения векторных операций) и в конце приводится пример решения задания с использованием библиотеки `sklearn`.

### Регрессия

**Линейная регрессия** – метод восстановления зависимости между двумя переменными:

$$y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n$$

Задача заключается в поиске такого набора параметров $$\theta$$, чтобы получать как можно лучшие результаты в предсказании $$y$$. В этом задании мы будем рассмотривать метод градиентного спуска для подбора параметров $$\theta$$ (существуют и другие методы, например, метод наименьших квадратов).

В качестве датасета нам послужат данные собранные в 1991 году Л. Уиллерманом (Willerman Lee) о 40 правополушарных студентах, изучавших курс психологии в Юго-Западном университете. Исследование, проведенное Уиллерманом, заключалось в поиске связи между размером мозга и коэффициентом интеллекта. Для исследования выбирались те студенты, которые не страдали алкоголизмом, потерей сознания, а также повреждениями мозга, эпилепсией или сердечной недостаточностью. Для определения размера мозга испытуемых исследователи использовали магнитно-резонансную томографию. Также каждому студенту было предложено пройти тесты Векслера. Информация о поле, весе и росте также была включена в данные. Таким образом, собранные Уиллерманом данные включают в себя следующие переменные:

 - пол (Gender) – мужской или женский;
 - значение комбинированного полного коэффициента интеллекта (FSIQ – Full Scale IQ);
 - коэффициент вербального интеллекта (VIQ – Verbal IQ);
 - баллы, набранные по тестам Векслера (PIQ – Performance IQ);
 - вес тела в фунтах (Weight);
 - рост в дюймах (Height);
 - количество пикселей на снимках МРТ (MRI_Count).

В качестве простого примера построим двумерную модель для определения (прогнозирования) коэффициента вербального интеллекта по коэффициенту комбинированного полного интеллекта:

```r
> brainSize <- read.csv("brain_size.csv")
> head(brainSize)
  Gender FSIQ VIQ PIQ Weight Height MRI_Count
1 Female  133 132 124    118   64.5    816932
2   Male  140 150 124     NA   72.5   1001121
3   Male  139 123 150    143   73.3   1038437
4   Male  133 129 128    172   68.8    965353
5 Female  137 132 134    147   65.0    951545
6 Female   99  90 110    146   69.0    928799
> X <- brainSize$FSIQ # input variables
> y <- brainSize$VIQ  # output variables
> plot(X, y,
    col = rgb(0.2, 0.4, 0.6, 0.4),
    main='Full Scale IQ vs Verbal IQ',
    xlab="FSIQ", ylab="VIQ")
```

<img src="/assets/images/11-ml/linreg1.png" width="70%">

### *Метод градиентного спуска*

Метод градиентного спуска это простой метод для поиска локального минимума функции. Подбор параметров $$\theta$$ происходит в соответствии со следующим правилом:

$$\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)$$

Где $$J(\theta)$$ называется целевой функцией (cost function), а $$\alpha$$ скоростью обучения (learning rate). Целевая функция вычисляется по следующей формуле:

$$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2 \rightarrow \frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

В результате подстановки получим следующее правило для пересчета параметров $$\theta$$:

$$\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i$$

В репозитории уже есть готовая реализация метода градиентного спуска на языке R, поэтому давайте воспользуемся ей:

```r
> library(repmis)
> source_data("https://github.com/Dementiy/LINK!!!")
> result <- gradientDescent(X, y, 0.00001, 100)
> result$theta
            [, 1]
[1, ] 0.008634596
[2, ] 0.987622436
```

Таким образом, итоговая модель для предсказания значения вербального интеллекта будет выглядеть следующим образом:

$$VIQ = 0.008634596 + 0.987622436 \times FSIQ$$

Например, в случае, когда комбинированный полный коэффициент интеллекта принимает значение в 100 баллов, то значение вербального интеллекта будет равно 98.77 баллам:

```r
> 0.008634596 + 0.987622436 * 100
[1] 98.77088
```

Построим графики сходимости целевой функции:

```r
> plot(result$cost_history, type='l', col='blue', lwd=2, 
    main='Cost function', ylab='cost', xlab='Iterations')
```

<img src="/assets/images/11-ml/linreg2.png" width="50%">

Следует заметить, что приведенная реализация метода градиентного спуска основывается на постоянном шаге $$\alpha$$, также мы положили число итераций равное 100, но на графике сходимости целевой функции хорошо видно, что мы можем уменьшить требуемое число итераций.

Вашей задачей является ...

```python
class LinearRegression:
    
    def __init__(self):
        pass

    def fit(self, X_train, y_train):
        pass

    def predict(self, X_test):
        pass
```

Пример с использованием библиотеки `sklearn`:

```python
>>> import sklearn
```

### Классификация

В общем виде задачу классификации можно представить следующим образом. Имеется множество объектов, которые разделены на классы по некоторым признакам. Например, успевающие студенты и отстающие студенты. В обучающей выборке задано конечное множество объектов и их признаков. Например, перечень всех студентов учебного заведения и все оценки по прошедшим и текущим дисциплинам. Для каждого из объектов обучающей выборки известно, к каким классам они относятся. Принадлежность же остальных объектов к классам неизвестна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества, то есть указать наименование (или номер) класса, к которому объект отнесён в результате применения алгоритма классификации.

```python
class LogisticRegression:
    
    def __init__(self):
        pass

    def fit(self, X_test, y_test):
        pass

    def predict(self, X_train):
        pass
```

### Кластеризация

**Кластерный анализ** – это способ группировки многомерных объектов, основанный на представлении результатов отдельных наблюдений точками подходящего геометрического пространства с последующим выделением групп как «сгустков» этих точек (кластеров, таксонов). Задачей такого разделения на группы является ухватить естественную структуру данных и абстрагироваться от индивидуальных характеристик каждого объекта к более общим признакам, которые объединяют эти объекты в кластеры. Например, кластеризация документов по их содержимому или кластеризация покупателей по их потребительской корзине и т.д. Так как заранее не известно по каким признакам следует объединять объекты в кластеры, то кластерный анализ относят к методам **обучения без учителя** (unsupervised learning).

Одним из наиболее простых и распространенных алгоритмов кластеризации является алгоритм k-средних (k-means), в котором каждый кластер представлен его центром (центроидом). k-средних можно описать следующими 4-мя шагами:

1. Выбрать k объектов как начальные центроиды.
2. Отнести остальные объекты к ближайшим центроидам.
3. Произвести перерасчет центроидов.
4. Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут «двигаться».

Пример поиска трех кластеров, используя алгоритм K-средних представлен на рисунках ниже:

![](/assets/images/11-ml/kmeans1.png)
![](/assets/images/11-ml/kmeans2.png)
![](/assets/images/11-ml/kmeans3.png)
![](/assets/images/11-ml/kmeans4.png)
![](/assets/images/11-ml/kmeans5.png)
![](/assets/images/11-ml/kmeans6.png)
![](/assets/images/11-ml/kmeans7.png)
![](/assets/images/11-ml/kmeans8.png)

Вашей задачей является написать класс `KMeans` со следующим интерфейсом:

```python
class KMeans:
    
    def __init__(self, n_clusters):
        pass

    def fit(self, X):
        pass

    def predict(self, y):
        pass
```

Для обучения модели будем использовать хорошо известный набор данных с лепестками цветов ириса:

```python
df = pd.read_csv('iris.csv')
X = df.loc[:, data.columns != 'Name'].as_matrix()
model = KMeans(3)
model.fit(X)
clusters = models._clusters
```

```python
plt.figure(figsize=(14,7))

# Create a colormap
colormap = np.array(['red', 'lime', 'black'])

# Plot Sepal
plt.subplot(1, 2, 1)
plt.scatter(df.SepalLength, df.SepalWidth, c=colormap[clusters], s=40)
plt.title('Sepal')

plt.subplot(1, 2, 2)
plt.scatter(df.PetalLength, df.PetalWidth, c=colormap[clusters], s=40)
plt.title('Petal')
```
