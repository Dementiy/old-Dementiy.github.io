<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Прогнозируем, классифицируем и кластеризуем (draft)</title>
  <meta name="description" content="Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://dementiy.github.io/2017/11/22/12-ml/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Ein Blog für freie Geister" href="https://dementiy.github.io/feed.xml">

  <link rel="icon" type="image/x-icon" href="/favicon.ico">


  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Прогнозируем, классифицируем и кластеризуем (draft)">
  <meta name="twitter:description" content="Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-111461883-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Ein Blog für freie Geister</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/practice/">Py&Go Practice</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/Dementiy/">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Прогнозируем, классифицируем и кластеризуем (draft)</h1>
    
    <p class="post-meta"><time datetime="2017-11-22T00:00:00+03:00" itemprop="datePublished">Nov 22, 2017</time> • 
  
  
    
      <a href="/categories/python/">python</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
      <a href="/categories/golang/">golang</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
      <a href="/categories/r/">R</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
      <a href="/categories/datascience/">datascience</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B8/">практики</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.</p>

<p>Перед выполнением каждого задания дается краткое описание рассматриваемой задачи (с минимальными математическими выкладками), приводится пример возможного решения на языке R, далее следует само задание, а именно реализация описываемых алгоритмов на языке Python (подразумевается использование модулей <code class="highlighter-rouge">numpy</code> и <code class="highlighter-rouge">pandas</code> для выполнения векторных операций) и в конце приводится пример решения задания с использованием библиотеки <code class="highlighter-rouge">sklearn</code>.</p>

<p><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"><img src="/assets/images/11-ml/sklearn_algos.png" width="100%" /></a></p>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h3 id="регрессия">Регрессия</h3>

<p><strong>Линейная регрессия</strong> – метод восстановления зависимости между двумя переменными:</p>

<script type="math/tex; mode=display">y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n</script>

<p>Задача заключается в поиске такого набора параметров <script type="math/tex">\theta</script>, чтобы получать как можно лучшие результаты в предсказании <script type="math/tex">y</script>. В этом задании мы будем рассмотривать метод градиентного спуска для подбора параметров <script type="math/tex">\theta</script> (существуют и другие методы, например, метод наименьших квадратов).</p>

<p>В качестве датасета нам послужат данные собранные в 1991 году Л. Уиллерманом (Willerman Lee) о 40 правополушарных студентах, изучавших курс психологии в Юго-Западном университете. Исследование, проведенное Уиллерманом, заключалось в поиске связи между размером мозга и коэффициентом интеллекта. Для исследования выбирались те студенты, которые не страдали алкоголизмом, потерей сознания, а также повреждениями мозга, эпилепсией или сердечной недостаточностью. Для определения размера мозга испытуемых исследователи использовали магнитно-резонансную томографию. Также каждому студенту было предложено пройти тесты Векслера. Информация о поле, весе и росте также была включена в данные. Таким образом, собранные Уиллерманом данные включают в себя следующие переменные:</p>

<ul>
  <li>пол (Gender) – мужской или женский;</li>
  <li>значение комбинированного полного коэффициента интеллекта (FSIQ – Full Scale IQ);</li>
  <li>коэффициент вербального интеллекта (VIQ – Verbal IQ);</li>
  <li>баллы, набранные по тестам Векслера (PIQ – Performance IQ);</li>
  <li>вес тела в фунтах (Weight);</li>
  <li>рост в дюймах (Height);</li>
  <li>количество пикселей на снимках МРТ (MRI_Count).</li>
</ul>

<p>В качестве простого примера построим двумерную модель для определения (прогнозирования) коэффициента вербального интеллекта VIQ по коэффициенту комбинированного полного интеллекта FSIQ.</p>

<p>Загрузим набор данных из репозитория и построим диаграмму рассеяния для соответствующих переменных:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">repmis</span><span class="p">)</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">source_data</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/Dementiy/pybook-assignments/master/homework11/brain_size.csv"</span><span class="p">)</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">head</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="w">
  </span><span class="n">Gender</span><span class="w"> </span><span class="n">FSIQ</span><span class="w"> </span><span class="n">VIQ</span><span class="w"> </span><span class="n">PIQ</span><span class="w"> </span><span class="n">Weight</span><span class="w"> </span><span class="n">Height</span><span class="w"> </span><span class="n">MRI_Count</span><span class="w">
</span><span class="m">1</span><span class="w"> </span><span class="n">Female</span><span class="w">  </span><span class="m">133</span><span class="w"> </span><span class="m">132</span><span class="w"> </span><span class="m">124</span><span class="w">    </span><span class="m">118</span><span class="w">   </span><span class="m">64.5</span><span class="w">    </span><span class="m">816932</span><span class="w">
</span><span class="m">2</span><span class="w">   </span><span class="n">Male</span><span class="w">  </span><span class="m">140</span><span class="w"> </span><span class="m">150</span><span class="w"> </span><span class="m">124</span><span class="w">     </span><span class="kc">NA</span><span class="w">   </span><span class="m">72.5</span><span class="w">   </span><span class="m">1001121</span><span class="w">
</span><span class="m">3</span><span class="w">   </span><span class="n">Male</span><span class="w">  </span><span class="m">139</span><span class="w"> </span><span class="m">123</span><span class="w"> </span><span class="m">150</span><span class="w">    </span><span class="m">143</span><span class="w">   </span><span class="m">73.3</span><span class="w">   </span><span class="m">1038437</span><span class="w">
</span><span class="m">4</span><span class="w">   </span><span class="n">Male</span><span class="w">  </span><span class="m">133</span><span class="w"> </span><span class="m">129</span><span class="w"> </span><span class="m">128</span><span class="w">    </span><span class="m">172</span><span class="w">   </span><span class="m">68.8</span><span class="w">    </span><span class="m">965353</span><span class="w">
</span><span class="m">5</span><span class="w"> </span><span class="n">Female</span><span class="w">  </span><span class="m">137</span><span class="w"> </span><span class="m">132</span><span class="w"> </span><span class="m">134</span><span class="w">    </span><span class="m">147</span><span class="w">   </span><span class="m">65.0</span><span class="w">    </span><span class="m">951545</span><span class="w">
</span><span class="m">6</span><span class="w"> </span><span class="n">Female</span><span class="w">   </span><span class="m">99</span><span class="w">  </span><span class="m">90</span><span class="w"> </span><span class="m">110</span><span class="w">    </span><span class="m">146</span><span class="w">   </span><span class="m">69.0</span><span class="w">    </span><span class="m">928799</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="o">$</span><span class="n">FSIQ</span><span class="w"> </span><span class="c1"># input variables</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="o">$</span><span class="n">VIQ</span><span class="w">  </span><span class="c1"># output variables</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w">
    </span><span class="n">main</span><span class="o">=</span><span class="s1">'Full Scale IQ vs Verbal IQ'</span><span class="p">,</span><span class="w">
    </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"FSIQ"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"VIQ"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/assets/images/11-ml/linreg1_new.png" width="70%" /></p>

<h3 id="метод-градиентного-спуска"><em>Метод градиентного спуска</em></h3>

<p>Метод градиентного спуска это простой метод для поиска локального минимума функции. Подбор параметров <script type="math/tex">\theta</script> происходит в соответствии со следующим правилом:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)</script>

<p>Где <script type="math/tex">J(\theta)</script> называется целевой функцией (cost function), а <script type="math/tex">\alpha</script> скоростью обучения (learning rate). Целевая функция вычисляется по следующей формуле:</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2 \rightarrow \frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<p>В результате подстановки получим следующее правило для пересчета параметров <script type="math/tex">\theta</script>:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Более подробно про метод градиентного спуска можно почитать <a href="http://mccormickml.com/2014/03/04/gradient-descent-derivation/">тут</a>.</p>
</div>

<script type="math/tex; mode=display">\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<p>В репозитории уже есть готовая реализация метода градиентного спуска на языке R, поэтому давайте воспользуемся ей:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="n">source</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/Dementiy/pybook-assignments/master/homework11/gradient_descent.R"</span><span class="p">)</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="m">0.00001</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">result</span><span class="o">$</span><span class="n">theta</span><span class="w">
            </span><span class="p">[,</span><span class="w"> </span><span class="m">1</span><span class="p">]</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="m">0.008634596</span><span class="w">
</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w"> </span><span class="m">0.987622436</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w">
    </span><span class="n">main</span><span class="o">=</span><span class="s1">'Full Scale IQ vs Verbal IQ'</span><span class="p">,</span><span class="w">
    </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"FSIQ"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"VIQ"</span><span class="p">)</span><span class="w">
</span><span class="o">&gt;</span><span class="w"> </span><span class="n">abline</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">result</span><span class="o">$</span><span class="n">theta</span><span class="p">[</span><span class="m">1</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="o">=</span><span class="n">result</span><span class="o">$</span><span class="n">theta</span><span class="p">[</span><span class="m">2</span><span class="p">])</span><span class="w">
</span></code></pre></div></div>

<p><img src="/assets/images/11-ml/linreg3.png" width="70%" /></p>

<p>Таким образом, итоговая модель для предсказания значения вербального интеллекта будет выглядеть следующим образом:</p>

<script type="math/tex; mode=display">VIQ = 0.008634596 + 0.987622436 \times FSIQ</script>

<p>Например, в случае, когда комбинированный полный коэффициент интеллекта принимает значение в 100 баллов, то значение вербального интеллекта будет равно 98.77 баллам:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="m">0.008634596</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.987622436</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="m">98.77088</span><span class="w">
</span></code></pre></div></div>

<p>Построим графики сходимости целевой функции:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="n">result</span><span class="o">$</span><span class="n">cost_history</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> 
    </span><span class="n">main</span><span class="o">=</span><span class="s1">'Cost function'</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s1">'cost'</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s1">'Iterations'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/assets/images/11-ml/linreg2.png" width="70%" /></p>

<p>Следует заметить, что приведенная реализация метода градиентного спуска основывается на постоянном шаге <script type="math/tex">\alpha</script>, также мы положили число итераций равное 100, но на графике сходимости целевой функции хорошо видно, что мы можем уменьшить требуемое число итераций.</p>

<h3 id="задание"><em>Задание</em></h3>

<p>Вашей задачей является реализовать метод градиентного спуска:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Имена методов выбраны в соответствии с тем как они названы в <code>sklearn</code>.</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GDRegression</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>Метод <code class="highlighter-rouge">fit</code> обучает модель на данных <code class="highlighter-rouge">X_train</code> (матрица признаков) и <code class="highlighter-rouge">y_train</code>(вектор ответов). Результатом работы этого метода являются два атрибута: <code class="highlighter-rouge">coef_</code> - вектор оценок для <script type="math/tex">\theta_i</script> (<script type="math/tex">i</script> принимает значения от 1 до <script type="math/tex">p</script>, где <script type="math/tex">p</script> - это количество признаков) и <code class="highlighter-rouge">intercept_</code> - оцененное значение для <script type="math/tex">\theta_0</script>.</p>

<p>Метод <code class="highlighter-rouge">predict</code> возвращает вектор прогнозов для новых данных.</p>

<p>Давайте рассмотрим пример использования класса <code class="highlighter-rouge">GDRegression</code>. Для начала загрузим набор данных, выведем пять первых наблюдений и построим диаграмму рассеивания:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"brain_size.csv"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
   <span class="n">Gender</span>  <span class="n">FSIQ</span>  <span class="n">VIQ</span>  <span class="n">PIQ</span>  <span class="n">Weight</span>  <span class="n">Height</span>  <span class="n">MRI_Count</span>
<span class="mi">0</span>  <span class="n">Female</span>   <span class="mi">133</span>  <span class="mi">132</span>  <span class="mi">124</span>   <span class="mf">118.0</span>    <span class="mf">64.5</span>     <span class="mi">816932</span>
<span class="mi">1</span>    <span class="n">Male</span>   <span class="mi">140</span>  <span class="mi">150</span>  <span class="mi">124</span>     <span class="n">NaN</span>    <span class="mf">72.5</span>    <span class="mi">1001121</span>
<span class="mi">2</span>    <span class="n">Male</span>   <span class="mi">139</span>  <span class="mi">123</span>  <span class="mi">150</span>   <span class="mf">143.0</span>    <span class="mf">73.3</span>    <span class="mi">1038437</span>
<span class="mi">3</span>    <span class="n">Male</span>   <span class="mi">133</span>  <span class="mi">129</span>  <span class="mi">128</span>   <span class="mf">172.0</span>    <span class="mf">68.8</span>     <span class="mi">965353</span>
<span class="mi">4</span>  <span class="n">Female</span>   <span class="mi">137</span>  <span class="mi">132</span>  <span class="mi">134</span>   <span class="mf">147.0</span>    <span class="mf">65.0</span>     <span class="mi">951545</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"FSIQ"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">"VIQ"</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Теперь создадим модель с параметрами <code class="highlighter-rouge">alpha=0.00001</code> и <code class="highlighter-rouge">n_iter=100</code> и обучим ее на всех имеющихся данных:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="c"># to keep it as DataFrame</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># X.insert(0, "Ones", np.ones(len(X)))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"VIQ"</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">GDRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.98762244</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="mf">0.0086345960432628564</span>
</code></pre></div></div>

<p>Итак, мы получили следующие оценки для параметров <script type="math/tex">\theta</script>: <script type="math/tex">\theta_0 = 0.00863459</script>, <script type="math/tex">\theta_1 = 0.98762244</script>. Не сложно заметить, что они совпадают с предыдущими значениями.</p>

<p>Давайте сделаем несколько прогнозов для имеющихся данных:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">array</span><span class="p">([</span><span class="mf">138.275366</span><span class="p">,</span> <span class="mf">137.287766</span><span class="p">,</span> <span class="mf">131.362163</span><span class="p">,</span> <span class="mf">135.312565</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="пример-с-использованием-библиотеки-sklearn"><em>Пример с использованием библиотеки sklearn</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegression</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"brain_size.csv"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"VIQ"</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">SGDRegression</span><span class="p">(</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.97827634</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.00892904</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">136.96761663</span><span class="p">,</span>  <span class="mf">135.98934029</span><span class="p">,</span>  <span class="mf">130.11968225</span><span class="p">,</span>  <span class="mf">134.03278761</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="выбор-модели"><em>Выбор модели</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">errors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
<span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">28</span><span class="p">,),</span> <span class="p">(</span><span class="mi">12</span><span class="p">,))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">GDRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.98536805</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.00886374</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rmse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="mf">6.7600543165795841</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
       <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:]</span>
       <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
       <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
       <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
       <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="p">[</span><span class="mf">10.3443</span><span class="p">,</span> <span class="mf">7.1500</span><span class="p">,</span> <span class="mf">6.5616</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Задания</strong>:</p>
<ol>
  <li>Используя кросс-валидацию построить лучшую модель.</li>
  <li>Реализовать метод стохастического градиентного спуска.</li>
</ol>

<h3 id="классификация">Классификация</h3>

<p>Вы уже встречались с задачей классфикации в работе <a href="http://127.0.0.1:4000/2017/11/22/06-hackernews/">«Персонализация новостной ленты Hacker News»</a>. В общем виде задачу классификации можно представить следующим образом: имеется множество объектов, которые разделены на классы по некоторым признакам. Например, успевающие студенты и отстающие студенты. В обучающей выборке задано конечное множество объектов и их признаков. Например, перечень всех студентов учебного заведения и все оценки по прошедшим и текущим дисциплинам. Для каждого из объектов обучающей выборки известно, к каким классам они относятся. Принадлежность же остальных объектов к классам неизвестна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества, то есть указать наименование (или номер) класса, к которому объект отнесён в результате применения алгоритма классификации.</p>

<h3 id="логистическая-регрессия"><em>Логистическая регрессия</em></h3>

<p>Давайте рассмотрим логистическую регрессию на примере набора данных цветов ириса.</p>

<p>Для простоты будем использовать только два признака <code class="highlighter-rouge">Sepal Width</code> и <code class="highlighter-rouge">Sepal Length</code>, а также два класса <code class="highlighter-rouge">Setosa</code> и <code class="highlighter-rouge">Versicolor</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="n">setosa</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">versicolor</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Sepal Length"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Sepal Width"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="n">setosa</span><span class="p">,</span> <span class="n">versicolor</span><span class="p">),</span> <span class="p">(</span><span class="s">"Setosa"</span><span class="p">,</span> <span class="s">"Versicolor"</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</code></pre></div></div>

<p>Итак, наша задача построить разделяющую границу (decision boundary), которая бы позволила нам отделить наблюдения одного класса (<code class="highlighter-rouge">Setosa</code>) от другого (<code class="highlighter-rouge">Versicolor</code>). Одним из напрашиваемых решений является представление зависимой переменной в виде линейной комбинации признаков (по аналогии с линейной регрессией):</p>

<script type="math/tex; mode=display">y_i = \theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i}</script>

<p>Предположим, что у нас есть наблюдение с признаками <script type="math/tex">SW = 3.5</script> и <script type="math/tex">SL = 5</script>. И кто-то сказал нам, что <script type="math/tex">\theta_0 = 1</script>, <script type="math/tex">\theta_1 = 2</script> и <script type="math/tex">\theta_2 = 4</script>, тогда:</p>

<script type="math/tex; mode=display">y_i = 1 + 2 * 3.5 + 4 * 5 = 28</script>

<p>Во-первых, как мы должны интерпретировать значене <script type="math/tex">28</script>? Во-вторых, модель в таком виде подходит для прогнозирования непрерывных значений, которые ограничены бесконечностью:</p>

<script type="math/tex; mode=display">-\infty \le \theta^{T}x \le +\infty</script>

<p>В действительности мы хотим построить модель, которая позволит нам прогнозировать бинарный отклик, а точнее вероятность отнесения наблюдения к одному из классов, то есть:</p>

<script type="math/tex; mode=display">P(y=1 \mid x) = f(\theta^{T}x)</script>

<script type="math/tex; mode=display">P(y=0 \mid x) = 1 - f(\theta^{T}x)</script>

<p>Где:</p>

<script type="math/tex; mode=display">0 \le f(\theta^{T}x) \le 1</script>

<p>Такой функцией <script type="math/tex">f</script> может быть <strong>логистическая функция</strong>:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = \frac{e^{\theta^{T}x}}{1 + e^{\theta^{T}x}} = \frac{1}{1 + e^{-\theta^{T}x}}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_values</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">y_values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</code></pre></div></div>

<p>Отметим, что значение вероятности, описываемое логистической функцией, ограничено диапазоном значений от 0 до 1, а также то, что большие изменения вероятности требуют больших изменений в <script type="math/tex">x</script>, чем для значений вероятности близких к 0.5.</p>

<p>Возвращаясь к нашему примеру:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i})}} = \frac{1}{1 + e^{-28}} = 0.99</script>

<p>Таким образом, с вероятностью 0.99 мы можем отнести наше наблюдение к классу <code class="highlighter-rouge">Setosa</code>.</p>

<p>Как мы будем находить параметры <script type="math/tex">\theta_i</script>? Для начала нам необходимо определиться с видом целевой функции. Так как наша модель прогнозирует вероятности, то мы можем использовать принцип максимального правдоподобия.</p>

<p>Для краткости, можем записать функцию распределения <script type="math/tex">y</script> при заданном <script type="math/tex">x</script> в следующем виде:</p>

<script type="math/tex; mode=display">P(y = y_i \mid x) = P(y_i = 1 \mid x_i)^{y_{i}} (1 - P(y_i = 1 \mid x_i))^{1 - y_{i}}</script>

<p>Фактически, это есть распределение Бернулли.</p>

<p>В зависимости от значения <script type="math/tex">y</script> одна из двух частей уравнения будет равна 1.</p>

<p>Для <script type="math/tex">y=1</script>:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = P(y_i = 1 \mid x_i)^1 (1 - P(y_i = 1 \mid x_i))^{1 - 1} = P(y_i = 1 \mid x_i)</script>

<p>Для <script type="math/tex">y=0</script>:</p>

<script type="math/tex; mode=display">P(y = 0 \mid x) = P(y_i = 1 \mid x_i)^0 (1 - P(y_i = 1 \mid x_i))^{1 - 0} = 1 - P(y_i = 1 \mid x_i)</script>

<p>Правдоподобие для <script type="math/tex">m</script> наблюдений можно записать как:</p>

<script type="math/tex; mode=display">L = \prod_{i=1}^{m}P(y = y_i \mid x_i) = \prod_{i=1}^{m}P(y = 1 \mid x_i)^{y_i}(1 - P(y_i = 1 \mid x_i))^{1 - y_i}</script>

<script type="math/tex; mode=display">\log L = \sum_{i=1}^{m} y_i \times \log(P(y = 1 \mid x_i)) + (1 - y_i) \times \log(1 - P(y_i = 1 \mid x_i))</script>

<p>Введем обозначение:</p>

<script type="math/tex; mode=display">p_i = P(y = 1 \mid x_i)</script>

<p>Тогда функция правдоподобия может быть записана как:</p>

<script type="math/tex; mode=display">\log L = \sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))</script>

<p>Таким образом, мы можем записать нашу целевую функцию:</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))</script>

<p>Чтобы найти параметры <script type="math/tex">\theta</script> мы можем воспользоваться методом градиентного спуска. Вспомним правило обновления весов:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)</script>

<p>Где <script type="math/tex">\frac{\partial}{\partial \theta}J(\theta)</script>:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Вывод производных см. <a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/">тут</a>.</p>
</div>

<script type="math/tex; mode=display">\frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}</script>

<script type="math/tex; mode=display">h_{\theta}(x) = f(\theta^Tx) = \frac{1}{1 + e^{-\theta^{T}x}}</script>

<script type="math/tex; mode=display">f(z) = \frac{1}{1 + e ^{-z}}</script>

<p>В результате подстановки получим следующее правило для пересчета параметров <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<h3 id="пример-с-использованием-библиотеки-sklearn-1"><em>Пример с использованием библиотеки sklearn</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span> <span class="c"># 30</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> <span class="c"># 1</span>
</code></pre></div></div>

<p>Давайте рассмотрим другой пример, для которого сгенеируем набор данных из 10 тысяч наблюдений:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mi">2306</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mf">0.76866666666666672</span>
</code></pre></div></div>

<p>Значение <script type="math/tex">0.7687</script> ничего не говорит нам об ошибках <a href="https://ru.wikipedia.org/wiki/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B8_%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B3%D0%BE_%D0%B8_%D0%B2%D1%82%D0%BE%D1%80%D0%BE%D0%B3%D0%BE_%D1%80%D0%BE%D0%B4%D0%B0">первого и второго рода</a>. Мы можем построить матрицу ошибок <script type="math/tex">C</script>, чтобы получить более полное представление.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">1129</span><span class="p">,</span>  <span class="mi">363</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">331</span><span class="p">,</span> <span class="mi">1177</span><span class="p">]])</span>
</code></pre></div></div>

<p>где:</p>
<ul>
  <li><script type="math/tex">C_{0,0} = 1129</script> – True Negatives (TN), истинное значение 0, предсказанное 0</li>
  <li><script type="math/tex">C_{1,0} = 331</script> – False Negatives (FN), истинное значение 1, предсказанное 0</li>
  <li><script type="math/tex">C_{0,1} = 363</script> – False Positives (FP), истинное значение 0, предсказанное 1</li>
  <li><script type="math/tex">C_{1,1} = 1177</script> – True Positives (TP), истинное значение 1, предсказанное 1</li>
</ul>

<p>И наконец мы можем построить ROC-кривую для визуальной оценки точности работы классификатора:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>

<span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkorange'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ROC curve (area = </span><span class="si">%0.2</span><span class="s">f)'</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'navy'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'ROC Curve'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="многоклассовая-классификация"><em>Многоклассовая классификация</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s">'ovr'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mi">42</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mf">0.93333333333333335</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">12</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">14</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">y_predict_proba</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="c"># Compute ROC curve and ROC AUC for each class</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">fpr</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tpr</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">all_y_test_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
    <span class="n">all_y_predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
        <span class="n">y_test_i</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">yi</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">y_test</span><span class="p">]</span>
        <span class="n">all_y_test_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">all_y_test_i</span><span class="p">,</span> <span class="n">y_test_i</span><span class="p">])</span>
        <span class="n">all_y_predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">all_y_predict_proba</span><span class="p">,</span> <span class="n">y_predict_proba</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]])</span>
        <span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test_i</span><span class="p">,</span> <span class="n">y_predict_proba</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">roc_auc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="c"># Compute micro-average ROC curve and ROC area</span>
    <span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">all_y_test_i</span><span class="p">,</span> <span class="n">all_y_predict_proba</span><span class="p">)</span>
    <span class="n">roc_auc</span><span class="p">[</span><span class="s">"average"</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">])</span>

    <span class="c"># Plot average ROC Curve</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s">'Average ROC curve (area = {0:0.2f})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">[</span><span class="s">"average"</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'deeppink'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="c"># Plot each individual ROC curve</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ROC curve of class {0} (area = {1:0.2f})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'k--'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Some extension of Receiver operating characteristic to multi-class'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="задание-1"><em>Задание</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<h3 id="кластеризация">Кластеризация</h3>

<p><strong>Кластерный анализ</strong> – это способ группировки многомерных объектов, основанный на представлении результатов отдельных наблюдений точками подходящего геометрического пространства с последующим выделением групп как «сгустков» этих точек (кластеров, таксонов). Задачей такого разделения на группы является ухватить естественную структуру данных и абстрагироваться от индивидуальных характеристик каждого объекта к более общим признакам, которые объединяют эти объекты в кластеры. Например, кластеризация документов по их содержимому или кластеризация покупателей по их потребительской корзине и т.д. Так как заранее не известно по каким признакам следует объединять объекты в кластеры, то кластерный анализ относят к методам <strong>обучения без учителя</strong> (unsupervised learning).</p>

<p>Одним из наиболее простых и распространенных алгоритмов кластеризации является алгоритм k-средних (k-means), в котором каждый кластер представлен его центром (центроидом).</p>

<ul>
  <li><script type="math/tex">k</script> означает число кластеров. Число кластеров не определяется автоматически и в каждом кластере может быть разное число объектов;</li>
  <li>k-средних использует двух шаговый эвристический подход к группированию похожих объектов: <strong>шаг присваивания</strong> и <strong>шаг обновления</strong>;</li>
  <li>схожесть (похожесть) объектов измеряется с помощью <strong>функции дистанции</strong>.</li>
</ul>

<h3 id="вычисление-дистанции-между-объектами"><em>Вычисление дистанции между объектами</em></h3>
<ul>
  <li>в методе k-средних значение каждого признака объекта воспринимается как координата в многомерном пространстве (например, если у нам известен только рост и вес человека, то мы имеем дело с двумерным объектом, где одна из координат это рост, а вторая - вес);</li>
  <li>схожесть объектов можно вычислить используя любую математическую функцию (метрику) схожести;</li>
  <li>обычно выбирается Евклидово расстояние:
 <script type="math/tex">dist(x,y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}</script></li>
</ul>

<h3 id="алгоритм-k-средних"><em>Алгоритм k-средних</em></h3>

<p>k-средних можно описать следующими 4-мя шагами:</p>
<ol>
  <li>Выбрать k объектов как начальные центроиды.</li>
  <li>Отнести остальные объекты к ближайшим центроидам.</li>
  <li>Произвести перерасчет центроидов.</li>
  <li>Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут «двигаться».</li>
</ol>

<p>Пример поиска трех кластеров, используя алгоритм K-средних представлен на рисунках ниже:</p>

<p><img src="/assets/images/11-ml/kmeans1.png" alt="" /><img src="/assets/images/11-ml/kmeans2.png" alt="" />
<img src="/assets/images/11-ml/kmeans3.png" alt="" /><img src="/assets/images/11-ml/kmeans4.png" alt="" />
<img src="/assets/images/11-ml/kmeans5.png" alt="" /><img src="/assets/images/11-ml/kmeans6.png" alt="" />
<img src="/assets/images/11-ml/kmeans7.png" alt="" /><img src="/assets/images/11-ml/kmeans8.png" alt="" /></p>

<h3 id="замечания-по-k-средних"><em>Замечания по k-средних</em></h3>
<ul>
  <li>k-средних эвристический алгоритм и не является детерминированным:
    <ul>
      <li>начальное положение центроидов оказывает существенное влияние на конечный результат;</li>
      <li>резуьтаты могут быть разными даже на одном и том же наборе данных.</li>
    </ul>
  </li>
  <li>так как используется расстояние для измерения схожести объектов, то все данные должны быть числовыми:
    <ul>
      <li>числовые данные должны быть приведены к единой шкале;</li>
      <li>могут возникнуть проблемы при кластеризации наборов данных состоящих из категориальных переменных.</li>
    </ul>
  </li>
  <li>иногда требуется знать предметную область и работать по методу проб и ошибок, чтобы получить нужные кластеры:
    <ul>
      <li>нужно заранее знать число кластеров <code class="highlighter-rouge">k</code>;</li>
      <li>может потребоваться оценка эксперта для ответа на вопрос <em>Являются ли полученные кластеры значимыми?</em></li>
    </ul>
  </li>
</ul>

<h3 id="как-выбирать-начальные-положения-центроидов"><em>Как выбирать начальные положения центроидов?</em></h3>
<ul>
  <li>можем случайным образом генерировать центры кластеров;</li>
  <li>или случайно выбрать k-объектов и назначить их центрами;</li>
  <li>но существуют и другие алгоритмы, которые позволяют увеличить точность и скорость сходимости k-средних, например:
    <blockquote>
      <p>Baswade, Nalwade. Selection of Initial Centroids for k-Means Algorithm</p>

      <ol>
        <li>From n objects calculate a point whose attribute values are average of n-objects attribute values. So first initial centroid is average on n-objects.</li>
        <li>Select next initial centroids from n-objects in such a way that the Euclidean distance of that object is maximum from other selected initial centroids.</li>
        <li>Repeat step 2 until we get k initial centroids. From these steps we will get initial centroids and with these initial centroids perform k-Means algorithm.</li>
      </ol>
    </blockquote>
  </li>
</ul>

<h3 id="подготовка-данных-к-кластеризации"><em>Подготовка данных к кластеризации</em></h3>

<p>Так как в методе k-средних значение каждого признака является координатой в многомерном пространстве, то все значения должны быть числовыми. Кроме того, хорошей практикой является приведение всех значений к единой шкале:</p>
<ul>
  <li>min-max нормализация:
 <script type="math/tex">x_{new} = \frac{x-min(x)}{max(x)-min(x)}</script></li>
  <li>z-score:
 <script type="math/tex">x_{new} = \frac{x - \mu}{\sigma} = \frac{x - mean(x)}{stddev(x)}</script></li>
  <li>введение порога:
 <script type="math/tex">x_{new} = 1 \mbox{ if } x \ge c \mbox{ else } 0</script></li>
</ul>

<p>Пример функций нормализации данных на языке R:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create normalization function</span><span class="w">
</span><span class="n">normalize</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">((</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># create z-score standartization function (exists in R, @see ?scale)</span><span class="w">
</span><span class="n">scale</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sd</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># create threshold function</span><span class="w">
</span><span class="n">threshold</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">ifelse</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Вашей задачей является написать класс <code class="highlighter-rouge">KMeans</code> со следующим интерфейсом:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KMeans</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>Для обучения модели будем использовать хорошо известный набор данных с лепестками цветов ириса:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.csv'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Name'</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">_clusters</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c"># Create a colormap</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s">'red'</span><span class="p">,</span> <span class="s">'lime'</span><span class="p">,</span> <span class="s">'black'</span><span class="p">])</span>

<span class="c"># Plot Sepal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">SepalLength</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">SepalWidth</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">clusters</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Sepal'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">PetalLength</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">PetalWidth</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">clusters</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Petal'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="как-выбирать-число-кластеров"><em>Как выбирать число кластеров?</em></h3>

<p>Одной из проблем кластеризации является верное определение числа кластеров. Одним из используемых методов, для решения этой проблемы, является отображение зависимости изменения суммы квадратов ошибок от выбранного числа кластеров:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mydata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bd.rates</span><span class="w">
</span><span class="n">wss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">mydata</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">mydata</span><span class="p">),</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">var</span><span class="p">))</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">15</span><span class="p">)</span><span class="w">
    </span><span class="n">wss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">kmeans</span><span class="p">(</span><span class="n">mydata</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="o">$</span><span class="n">withinss</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">wss</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span><span class="w"> 
           </span><span class="n">xlab</span><span class="o">=</span><span class="s1">'Number of clusters'</span><span class="p">,</span><span class="w"> 
           </span><span class="n">ylab</span><span class="o">=</span><span class="s1">'Within groups sum of squares'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>


  </div>

  
    <div class="post-comments" itemprop="comment">
      

    </div>
  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://dementiy.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
