<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Прогнозируем, классифицируем и кластеризуем</title>
  <meta name="description" content="Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://dementiy.github.io/2017/11/22/12-ml/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Ein Blog für freie Geister" href="https://dementiy.github.io/feed.xml">

  <link rel="icon" type="image/x-icon" href="/favicon.ico">


  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Прогнозируем, классифицируем и кластеризуем">
  <meta name="twitter:description" content="Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-111461883-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Ein Blog für freie Geister</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/practice/">Py&Go Practice</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/Dementiy/">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Прогнозируем, классифицируем и кластеризуем</h1>
    
    <p class="post-meta"><time datetime="2017-11-22T00:00:00+03:00" itemprop="datePublished">Nov 22, 2017</time> • 
  
  
    
      <a href="/categories/python/">python</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
      <a href="/categories/golang/">golang</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
      <a href="/categories/r/">R</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
      <a href="/categories/datascience/">datascience</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B8/">практики</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Эта работа знакомит вас с некоторыми задачами из области машинного обучения, а именно задачами регрессии, классификации и кластеризации.</p>

<p>Перед выполнением каждого задания дается краткое описание рассматриваемой задачи (с минимальными математическими выкладками), приводится пример возможного решения на языке R, далее следует само задание, а именно реализация описываемых алгоритмов на языке Python (подразумевается использование модулей <code class="highlighter-rouge">numpy</code> и <code class="highlighter-rouge">pandas</code> для выполнения векторных операций) и в конце приводится пример решения задания с использованием библиотеки <code class="highlighter-rouge">sklearn</code>.</p>

<p><a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"><img src="/assets/images/11-ml/sklearn_algos.png" width="100%" /></a></p>

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h3 id="регрессия">Регрессия</h3>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Подробнее про линейную регрессию можно почитать в <a href="">«Заметки по линейной регрессии»</a>.</p>
</div>

<p>Линейная регрессия это метод восстановления зависимости между двумя переменными:</p>

<script type="math/tex; mode=display">y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ... + \theta_nx_n</script>

<p>Задача заключается в поиске такого набора параметров <script type="math/tex">\theta</script>, чтобы получать как можно лучшие результаты в предсказании <script type="math/tex">y</script>. В этом задании мы будем рассмотривать метод градиентного спуска для подбора параметров <script type="math/tex">\theta</script> (существуют и другие методы, например, метод наименьших квадратов).</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Описание набора данных взято из курса <a href="http://www.uic.unn.ru/~zny/ml/Others/ml_pop.pdf">Н.Ю.Золотых</a>.</p>
</div>

<p>В качестве набора данных мы будем использовать <a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html">The Boston Housing Dataset</a>.  База содержит информацию о
загородных домах близ Бостона, собранную службой переписи населения США. Данные были собраны в 1970-х годах. Информация агрегирована: территория поделена на участки и дома, стоящие на одном участке, собраны в группы. Нужно оценить среднюю цену дома. Таким образом, объектами являются сами эти группы. Их общее количество — 506. В качестве признаков рассматриваются:</p>

<ul>
  <li><code class="highlighter-rouge">CRIM</code> — уровень преступности на душу населения;</li>
  <li><code class="highlighter-rouge">ZN</code> — процент земли, застроенной жилыми домами (только для участков площадью свыше 25000 кв. - футов);</li>
  <li><code class="highlighter-rouge">INDUS</code> — процент деловой застройки;</li>
  <li><code class="highlighter-rouge">CHAS</code> — 1, если участок граничит с рекой; 0 в противном случае (бинарный признак);</li>
  <li><code class="highlighter-rouge">NOX</code> — концентрация оксида азота, деленная на 10^7;</li>
  <li><code class="highlighter-rouge">RM</code> — среднее число комнат (по всем домам рассматриваемого участка);</li>
  <li><code class="highlighter-rouge">AGE</code> — процент домов, построенных до 1940 г. и занимаемых владельцами;</li>
  <li><code class="highlighter-rouge">DIS</code> — взвешенное расстояние до 5 деловых центров Бостона;</li>
  <li><code class="highlighter-rouge">RAD</code> — индекс удаленности до радиальных магистралей;</li>
  <li><code class="highlighter-rouge">TAX</code> — величина налога в $10000;</li>
  <li><code class="highlighter-rouge">PTRATIO</code> — количество учащихся, приходящихся на одного учителя (по городу);</li>
  <li><code class="highlighter-rouge">B = 1000(AA − 0.63)^2</code>, где <code class="highlighter-rouge">AA</code> — доля афро-американцев;</li>
  <li><code class="highlighter-rouge">LSTAT</code> — процент жителей с низким социальным статусом.</li>
</ul>

<p>Признак <code class="highlighter-rouge">CHAS</code> — бинарный, остальные — количественные. Выходом является переменная <code class="highlighter-rouge">MEDV</code>, равная медианному значению цены строения (по всем домам участка) в $1000.</p>

<p>Для начала загрузим набор данных и выведем пять первых наблюдений:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s">'MEDV'</span><span class="p">]</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \
0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   
1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   
2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   
3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   
4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   

   PTRATIO       B  LSTAT  MEDV  
0     15.3  396.90   4.98  24.0  
1     17.8  396.90   9.14  21.6  
2     17.8  392.83   4.03  34.7  
3     18.7  394.63   2.94  33.4  
4     18.7  396.90   5.33  36.2 
</code></pre></div></div>

<p>Мы не будем уделять время разведночному анализу данных, но для желающих поближе познакомиться с набором данных можно начать с этой <a href="http://www.neural.cz/dataset-exploration-boston-house-pricing.html">статьи</a>.</p>

<h3 id="простая-линейная-регрессия"><em>Простая линейная регрессия</em></h3>

<p>Мы начнем с построения простой линейной модели с одной переменной. Переменную выберем по значению коэффициента корреляции Пирсона с целевым признаком <code class="highlighter-rouge">MEDV</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pearson</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'pearson'</span><span class="p">)</span>
<span class="n">pearson</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">]</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"MEDV"</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RM         0.695360
ZN         0.360445
B          0.333461
DIS        0.249929
CHAS       0.175260
AGE       -0.376955
RAD       -0.381626
CRIM      -0.385832
NOX       -0.427321
TAX       -0.468536
INDUS     -0.483725
PTRATIO   -0.507787
LSTAT     -0.737663
Name: MEDV, dtype: float64
</code></pre></div></div>

<div class="admonition note">
  <p class="first admonition-title"><strong>Вопрос</strong></p>
  <p class="last">Объясните почему значение коэффициента корреляции между <code>MEDV</code> и переменными <code>RM</code> и <code>LSTAT</code> имеет противоположные знаки (исходя из описания переменных).</p>
</div>

<p>Наиболее высокий коэффициент корреляции у двух переменных: <code class="highlighter-rouge">RM</code> (среднее число комнат) и <code class="highlighter-rouge">LSTAT</code> (процент жителей с низким социальным статусом). Выберем переменную <code class="highlighter-rouge">RM</code> для объяснения <code class="highlighter-rouge">MEDV</code>.</p>

<p>Давайте построим диаграмму рассеяния и визуально оценим взаимосвязь между средним числом комнат на участке и средней ценой дома на этом участке:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"RM"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s">'scatter'</span><span class="p">,</span> <span class="n">joint_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'alpha'</span><span class="p">:</span><span class="mf">0.5</span><span class="p">});</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_jointplot.png" /></p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про аномалии и выбросы можно почитать <a href="https://alexanderdyakonov.wordpress.com/2017/04/19/%D0%BF%D0%BE%D0%B8%D1%81%D0%BA-%D0%B0%D0%BD%D0%BE%D0%BC%D0%B0%D0%BB%D0%B8%D0%B9-anomaly-detection/">блоге Александра Дьяконова</a>.</p>
</div>
<p>Из диаграммы рассеяния хорошо видно, что имеет место положительная линейная связь между переменными <code class="highlighter-rouge">RM</code> и <code class="highlighter-rouge">MEDV</code>, то есть дома с большим числом комнат имеют более высокую цену. Также можем сделать следующие предположения:</p>
<ul>
  <li>16 наблюдений <code class="highlighter-rouge">MEDV</code> имеют значение 50.0. Скорее всего эти наблюдения содержали пропуски или не было возможности указать действительную цену дома, поэтому эти наблюдения могут быть исключены из набора данных.</li>
  <li>одно наблюдение признака <code class="highlighter-rouge">RM</code> имеет значение 8.78. Это наблюдение может рассматриваться как выброс и может быть исключено из набора данных;</li>
  <li>два наблюдение признака <code class="highlighter-rouge">RM</code> имеют значения меньше 4. Наблюдения можно рассматривать как выбросы и должны быть исключены из набора данных.</li>
</ul>

<p>Итак, модель которую мы будем строить имеет следующий вид:</p>

<script type="math/tex; mode=display">MEDV = \theta_0 + \theta_1 \times RM</script>

<p>Нам необходимо найти оптимальные значения для параметров <script type="math/tex">\theta</script>.</p>

<h3 id="метод-градиентного-спуска"><em>Метод градиентного спуска</em></h3>

<p>Метод градиентного спуска это простой метод для поиска локального минимума функции. Подбор параметров <script type="math/tex">\theta</script> происходит в соответствии со следующим правилом:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)</script>

<p>Где <script type="math/tex">J(\theta)</script> называется целевой функцией (cost function), а <script type="math/tex">\alpha</script> скоростью обучения (learning rate). Целевая функция вычисляется по следующей формуле:</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x_i) - y_i)^2 \rightarrow \frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<p>В результате подстановки получим следующее правило для пересчета параметров <script type="math/tex">\theta</script>:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Более подробно про метод градиентного спуска можно почитать <a href="http://mccormickml.com/2014/03/04/gradient-descent-derivation/">тут</a>.</p>
</div>

<script type="math/tex; mode=display">\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<p>Вашей задачей является реализовать метод градиентного спуска:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Имена методов выбраны в соответствии с тем как они названы в <code>sklearn</code>.</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GDRegressor</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>Метод <code class="highlighter-rouge">fit</code> обучает модель на данных <code class="highlighter-rouge">X_train</code> (матрица признаков) и <code class="highlighter-rouge">y_train</code>(вектор ответов). Результатом работы этого метода являются два атрибута: <code class="highlighter-rouge">coef_</code> - вектор оценок для <script type="math/tex">\theta_i</script> (<script type="math/tex">i</script> принимает значения от 1 до <script type="math/tex">p</script>, где <script type="math/tex">p</script> - это количество признаков) и <code class="highlighter-rouge">intercept_</code> - оцененное значение для <script type="math/tex">\theta_0</script>.</p>

<p>Метод <code class="highlighter-rouge">predict</code> возвращает вектор прогнозов для новых данных.</p>

<p>Давайте рассмотрим пример использования класса <code class="highlighter-rouge">GDRegressor</code>. Предварительно разобьем выборку на обучающую и тестовую и обучим модель на обучающей выборке, положив значения для параметров <code class="highlighter-rouge">max_iter=2000</code> и <code class="highlighter-rouge">alpha=0.04</code>.</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про кросс-валидацию с примерами можно почитать <a href="https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6">тут</a>.<br /><br />У вас значения <code>model.coef_</code> и <code>model.intercept_</code> могут немного отличаться от полученных мной.</p>
</div>

<div class="admonition note">
  <p class="first admonition-title"><strong>Вопрос</strong></p>
  <p class="last">Дайте интерпретацию полученным значениям для <code>model.coef_</code> и <code>model.intercept_</code>.</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s">"RM"</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">GDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.04</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mf">7.07554766</span><span class="p">]),</span> <span class="o">-</span><span class="mf">21.792447302876575</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">data</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s">'scatter'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"RM"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"MEDV"</span><span class="p">);</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_train</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="s">'r'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_fitted1.png" /></p>

<h3 id="оценка-качества-модели"><em>Оценка качества модели</em></h3>

<p>Итак, мы нашли коэффициенты <script type="math/tex">\theta</script>, теперь надо как-то оценить качество работы полученной модели. Для этого мы воспользуемся коэффициентом детерминации <script type="math/tex">R^2</script>:</p>

<script type="math/tex; mode=display">R^2 = 1 - \frac{\sum_{i=1}^{m}(y_i - \hat{y_i})^2}{\sum_{i=1}^{m}(y_i - \overline{y})^2}</script>

<p>и среднеквадратичной ошибкой:</p>

<script type="math/tex; mode=display">RMSE = \sqrt{\frac{\sum_{i=1}^{m}(\hat{y_i} - y_i)^2}{m}}</script>

<p><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8">Коэффициент детерминации</a> это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью. Более точно — это единица минус доля необъяснённой дисперсии (дисперсии случайной ошибки модели, или условной по признакам дисперсии зависимой переменной) в дисперсии зависимой переменной.</p>

<p>Среднеквадратичная ошибка характеризует отклонение реальных данных от линии регрессии и измеряется в тех же единицах, что и зависимая переменная (<script type="math/tex">y</script>).</p>

<p>Вашей задачей является написать две функции для вычисления коэфиициента детерминации и среднеквадратичной ошибки:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">""" Root mean squared error """</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">r_squared</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">""" R-squared score """</span>
    <span class="k">pass</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rmse</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="mf">6.500256917031142</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">r_squared</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="mf">0.488226294688234</span>
</code></pre></div></div>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про анализ регрессионных остатков можно почитать <a href="http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/">тут</a> и <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D1%85_%D0%BE%D1%81%D1%82%D0%B0%D1%82%D0%BA%D0%BE%D0%B2_(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80)">тут</a>.</p>
</div>

<p>Построим график истинных значений и прогнозов:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Prices: $Y_i$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Predicted prices: $</span><span class="err">\</span><span class="s">hat{Y}_i$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Prices vs Predicted prices: $Y_i$ vs $</span><span class="err">\</span><span class="s">hat{Y}_i$"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_predicted1.png" /></p>

<p>Также построим график остатков (residual plot):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">Y_pred</span><span class="o">-</span><span class="n">Y_test</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_residuals1.png" /></p>

<p>Можем ли мы улучшить качество нашей модели? Мы выбрали значения для параметров <code class="highlighter-rouge">max_iter</code> и <code class="highlighter-rouge">alpha</code> «случайным» образом. Воспользуйтесь функцией <code class="highlighter-rouge">plot_cost_function</code> для того, чтобы найти оптимальные значения для числа итераций <code class="highlighter-rouge">max_iter</code> и параметра <code class="highlighter-rouge">alpha</code>. Например:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">GDRegressor</span><span class="p">,</span> <span class="n">max_iters</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.04</span><span class="p">)</span>
</code></pre></div></div>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Обратите внимание, что строится график зависимости MSE (RMSE^2) от номера текущей итерации.</p>
</div>

<p><img src="/assets/images/11-ml/lr_costfunc1.png" /></p>

<p>Очевидно, что на двух тысячах итераций и скорости обучения <script type="math/tex">0.04</script> алгоритм еще не сошелся. Попробуйте увеличить число итераций или изменить значение <code class="highlighter-rouge">alpha</code> (или и то и другое). В итоге вы должны получить значение для среднеквадратичной ошибки <script type="math/tex">6.433</script>, а для коэффициента детерминации <script type="math/tex">0.498</script>.</p>

<p>В начале мы сделали несколько предположений о выбросах, давайте проверим одно из них. Удалите из набора данных наблюдения, значения которых для признака <code class="highlighter-rouge">MEDV</code> равны 50. И снова воспользуйтесь функцией <code class="highlighter-rouge">plot_cost_function</code> для определения числа итераций и скорости обучения. Также ответье на вопрос «Улучшилось ли качество модели?».</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Про важность нормализации признаков можно почитать <a href="https://www.robertoreif.com/blog/2017/12/16/importance-of-feature-scaling-in-data-modeling-part-1-h8nla">тут</a> и <a href="http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#z-score-standardization-or-min-max-scaling">тут</a>.</p>
</div>

<p>Вы должны были обратить внимание, что увеличилось необходимое число итераций для сходимости алгоритма. Одним из способов повлиять на это является нормализация признаков. Мы будем использовать один из самых простых и распространенных способов нормализации - Standart Scaling (Z-score normalization):</p>

<script type="math/tex; mode=display">x' = \frac{x - \overline{x}}{\sigma_x}</script>

<p>Напишите функцию <code class="highlighter-rouge">z_scaler</code> для нормализации значений:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">z_scaler</span><span class="p">(</span><span class="n">feature</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">z_scaler</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y_scaled</span> <span class="o">=</span> <span class="n">z_scaler</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Снова воспользуйтесь функцией <code class="highlighter-rouge">plot_cost_function</code>, но начните с небольшого числа итераций и при необходимости постепенно его увеличивайте.</p>

<h3 id="пример-с-использованием-библиотеки-sklearn"><em>Пример с использованием библиотеки sklearn</em></h3>

<p>Проделайте теже шаги с использованием библиотеки <code class="highlighter-rouge">sklearn</code>: с помощью функции <code class="highlighter-rouge">plot_cost_function</code> с параметром <code class="highlighter-rouge">normalize=True</code> найдите оптимальное число итераций и значение для скорости обучения, вычислите значение среднеквадратичной ошибки и коэффициента детерминации, постройте графики остатков.</p>

<p>Для того, чтобы построить модель:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Обратите внимание, что скорость обучение задается с помощью параметра <code>eta0</code>.</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span>
        <span class="n">loss</span><span class="o">=</span><span class="s">'squared_loss'</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="s">'constant'</span><span class="p">,</span>
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">)</span>
</code></pre></div></div>

<p>Чтобы нормализовать значения для обучения и валидации модели воспользуйтесь <code class="highlighter-rouge">StandardScaler</code> из библиотеки <code class="highlighter-rouge">sklearn</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Y_train</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y_test</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>Чтобы найти среднеквадратичную ошибку и коэффициент детерминации:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>В результате вы должны получить следующие значения:</p>
<ul>
  <li>коэффициенты модели - <script type="math/tex">\theta_0 = 5.85827955e-05, \theta_1 = 0.64210931</script>;</li>
  <li>коэффициент детерминации - <script type="math/tex">0.561</script>;</li>
  <li>среднеквадратичная ошибка - <script type="math/tex">0.439</script>.</li>
</ul>

<h3 id="многомерная-линейная-регрессия"><em>Многомерная линейная регрессия</em></h3>

<p>До этого мы рассматривали только один признак - среднее число комнат на участке. Давайте в нашу модель добавим еще один признак - процент жителей с низким социальным статусом (<code class="highlighter-rouge">LSTAT</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"LSTAT"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s">'scatter'</span><span class="p">,</span> <span class="n">joint_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'alpha'</span><span class="p">:</span><span class="mf">0.5</span><span class="p">});</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_jointplot2.png" /></p>

<p>Снова воспользуемся функцией <code class="highlighter-rouge">plot_cost_function</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_filtered</span> <span class="o">=</span> <span class="n">data</span><span class="p">[(</span><span class="n">data</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">50</span><span class="p">)][[</span><span class="s">"RM"</span><span class="p">,</span> <span class="s">"LSTAT"</span><span class="p">]]</span>
<span class="n">y_filtered</span> <span class="o">=</span> <span class="n">data</span><span class="p">[(</span><span class="n">data</span><span class="p">[</span><span class="s">"MEDV"</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">50</span><span class="p">)][</span><span class="s">"MEDV"</span><span class="p">]</span>

<span class="n">plot_cost_function</span><span class="p">(</span>
    <span class="n">X_filtered</span><span class="p">,</span>
    <span class="n">y_filtered</span><span class="p">,</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">SGDRegressor</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_iters</span><span class="o">=</span><span class="err">???</span><span class="p">,</span> <span class="c"># PUT YOUR VALUE HERE</span>
    <span class="n">eta0</span><span class="o">=</span><span class="err">???</span>       <span class="c"># PUT YOUR VALUE HERE</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Постройте и обучите модель для найденных значений <code class="highlighter-rouge">max_iter</code> и <code class="highlighter-rouge">alpha</code>. Вы должны получить следующие значения для среднеквадратичной ошибки и коэффициента детерминации:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">),</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
<span class="p">(</span><span class="mf">0.29456340651451396</span><span class="p">,</span> <span class="mf">0.7054365934854863</span><span class="p">)</span>
</code></pre></div></div>

<p>Добавление нового признака значительно снизило среднеквадратичную ошибку и повысило описательную способность нашей модели. Давайте построим график остатков:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">Y_pred</span> <span class="o">-</span> <span class="n">Y_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_residuals2.png" /></p>

<p>Из графика хорошо видно, что есть нелинейная зависимость в остатках. Это связано с тем, что процент жителей с низким социальным статусом нелинейно зависит от средней цены на дом и среднего числа комнат:</p>

<p><img src="/assets/images/11-ml/lr_scatter3d.png" /></p>

<p>Эту зависимость наша модель в настоящий момент не описывает. Таким образом, мы должны добавить в нашу модель нелинейные признаки.</p>

<h3 id="добавление-полиномиальных-признаков"><em>Добавление полиномиальных признаков</em></h3>

<p>Итак, для описания нелинейной зависимости мы будем использовать полиномиальную модель. Предположим, что полинома четвертой степени достаточно для описания зависимости. Возможно будет достаточно полинома второй или третьей степени, поэтому, чтобы «занулить» некоторые коэффициенты <script type="math/tex">\theta</script>, мы будем использовать L1-регуляризацию.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_filtered</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_cost_function</span><span class="p">(</span>
    <span class="n">X_filtered</span><span class="p">,</span>
    <span class="n">y_filtered</span><span class="p">,</span>
    <span class="n">SGDRegressor</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_iters</span><span class="o">=</span><span class="err">???</span><span class="p">,</span> <span class="c"># PUT YOUR VALUE HERE</span>
    <span class="n">eta0</span><span class="o">=</span><span class="err">???</span><span class="p">,</span>      <span class="c"># PUT YOUR VALUE HERE</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span>
        <span class="s">'penalty'</span><span class="p">:</span> <span class="s">'l1'</span><span class="p">,</span>
        <span class="s">'alpha'</span><span class="p">:</span> <span class="err">???</span> <span class="c"># PUT YOUR VALUE HERE</span>
    <span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Постройте и обучите модель для найденных значений <code class="highlighter-rouge">max_iter</code> и <code class="highlighter-rouge">alpha</code>. Вы должны получить примерно следующие значения для среднеквадратичной ошибки и коэффициента детерминации:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">),</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
<span class="p">(</span><span class="mf">0.17508905829300977</span><span class="p">,</span> <span class="mf">0.8249109417069903</span><span class="p">)</span>
</code></pre></div></div>

<p>И снова мы значительно снизили значение среднеквадратичной ошибки и повысили описательную способность нашей модели. Давайте посмотрим на график остатков:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">Y_pred</span><span class="o">-</span><span class="n">Y_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/assets/images/11-ml/lr_residuals3.png" /></p>

<p>Также давайте посмотрим на полученные значения коэффициентов модели:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span>
<span class="c">#        RM           LSTAT        RM^2         RM*LSTAT     LSTAT^2</span>
<span class="p">(</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
<span class="c">#        RM^3         RM^2*LSTAT   RM*LSTAT^2   LSTAT^3      RM^4</span>
         <span class="mf">0.62890582</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>
<span class="c">#        RM^3*LSTAT   RM^2*LSTAT^2 RM*LSTAT^3   LSTAT^4</span>
        <span class="o">-</span><span class="mf">0.52099058</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">,</span>  <span class="mf">0.0317591</span> <span class="p">]),</span>
 <span class="n">array</span><span class="p">([</span><span class="mf">0.0018161</span><span class="p">]))</span>
</code></pre></div></div>

<p>Итак, если мы подставим значения ненулевых коэффициентов, то получим следующиую модель:</p>

<script type="math/tex; mode=display">\text{MEDV} = 0.0018161 + 0.62890582 \times \text{RM}^3 - 0.52099058 \times \text{RM}^3 \times \text{LSTAT} + 0.0317591 \times \text{LSTAT}^4</script>

<p>Модель стала немного сложнее, появился также эффект взаимодействия между признаками <code class="highlighter-rouge">RM</code> и <code class="highlighter-rouge">LSTAT</code>.</p>

<p>Определите какие еще признаки имеет смысл включить в модель.</p>

<h3 id="классификация">Классификация</h3>

<p>Вы уже встречались с задачей классфикации в работе <a href="http://127.0.0.1:4000/2017/11/22/06-hackernews/">«Персонализация новостной ленты Hacker News»</a>. В общем виде задачу классификации можно представить следующим образом: имеется множество объектов, которые разделены на классы по некоторым признакам. Например, успевающие студенты и отстающие студенты. В обучающей выборке задано конечное множество объектов и их признаков. Например, перечень всех студентов учебного заведения и все оценки по прошедшим и текущим дисциплинам. Для каждого из объектов обучающей выборки известно, к каким классам они относятся. Принадлежность же остальных объектов к классам неизвестна. Требуется построить алгоритм, способный классифицировать произвольный объект из исходного множества, то есть указать наименование (или номер) класса, к которому объект отнесён в результате применения алгоритма классификации.</p>

<h3 id="логистическая-регрессия"><em>Логистическая регрессия</em></h3>

<p>Давайте рассмотрим логистическую регрессию на примере набора данных цветов ириса.</p>

<p>Для простоты будем использовать только два признака <code class="highlighter-rouge">Sepal Width</code> и <code class="highlighter-rouge">Sepal Length</code>, а также два класса <code class="highlighter-rouge">Setosa</code> и <code class="highlighter-rouge">Versicolor</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="n">setosa</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">versicolor</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">50</span><span class="p">:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Sepal Length"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Sepal Width"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="n">setosa</span><span class="p">,</span> <span class="n">versicolor</span><span class="p">),</span> <span class="p">(</span><span class="s">"Setosa"</span><span class="p">,</span> <span class="s">"Versicolor"</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</code></pre></div></div>

<p>Итак, наша задача построить разделяющую границу (decision boundary), которая бы позволила нам отделить наблюдения одного класса (<code class="highlighter-rouge">Setosa</code>) от другого (<code class="highlighter-rouge">Versicolor</code>). Одним из напрашиваемых решений является представление зависимой переменной в виде линейной комбинации признаков (по аналогии с линейной регрессией):</p>

<script type="math/tex; mode=display">y_i = \theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i}</script>

<p>Предположим, что у нас есть наблюдение с признаками <script type="math/tex">SW = 3.5</script> и <script type="math/tex">SL = 5</script>. И кто-то сказал нам, что <script type="math/tex">\theta_0 = 1</script>, <script type="math/tex">\theta_1 = 2</script> и <script type="math/tex">\theta_2 = 4</script>, тогда:</p>

<script type="math/tex; mode=display">y_i = 1 + 2 * 3.5 + 4 * 5 = 28</script>

<p>Во-первых, как мы должны интерпретировать значене <script type="math/tex">28</script>? Во-вторых, модель в таком виде подходит для прогнозирования непрерывных значений, которые ограничены бесконечностью:</p>

<script type="math/tex; mode=display">-\infty \le \theta^{T}x \le +\infty</script>

<p>В действительности мы хотим построить модель, которая позволит нам прогнозировать бинарный отклик, а точнее вероятность отнесения наблюдения к одному из классов, то есть:</p>

<script type="math/tex; mode=display">P(y=1 \mid x) = f(\theta^{T}x)</script>

<script type="math/tex; mode=display">P(y=0 \mid x) = 1 - f(\theta^{T}x)</script>

<p>Где:</p>

<script type="math/tex; mode=display">0 \le f(\theta^{T}x) \le 1</script>

<p>Такой функцией <script type="math/tex">f</script> может быть <strong>логистическая функция</strong>:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = \frac{e^{\theta^{T}x}}{1 + e^{\theta^{T}x}} = \frac{1}{1 + e^{-\theta^{T}x}}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_values</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">y_values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</code></pre></div></div>

<p>Отметим, что значение вероятности, описываемое логистической функцией, ограничено диапазоном значений от 0 до 1, а также то, что большие изменения вероятности требуют больших изменений в <script type="math/tex">x</script>, чем для значений вероятности близких к 0.5.</p>

<p>Возвращаясь к нашему примеру:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 SW_{i} + \theta_2 SL_{i})}} = \frac{1}{1 + e^{-28}} = 0.99</script>

<p>Таким образом, с вероятностью 0.99 мы можем отнести наше наблюдение к классу <code class="highlighter-rouge">Setosa</code>.</p>

<p>Как мы будем находить параметры <script type="math/tex">\theta_i</script>? Для начала нам необходимо определиться с видом целевой функции. Так как наша модель прогнозирует вероятности, то мы можем использовать принцип максимального правдоподобия.</p>

<p>Для краткости, можем записать функцию распределения <script type="math/tex">y</script> при заданном <script type="math/tex">x</script> в следующем виде:</p>

<script type="math/tex; mode=display">P(y = y_i \mid x) = P(y_i = 1 \mid x_i)^{y_{i}} (1 - P(y_i = 1 \mid x_i))^{1 - y_{i}}</script>

<p>Фактически, это есть распределение Бернулли.</p>

<p>В зависимости от значения <script type="math/tex">y</script> одна из двух частей уравнения будет равна 1.</p>

<p>Для <script type="math/tex">y=1</script>:</p>

<script type="math/tex; mode=display">P(y = 1 \mid x) = P(y_i = 1 \mid x_i)^1 (1 - P(y_i = 1 \mid x_i))^{1 - 1} = P(y_i = 1 \mid x_i)</script>

<p>Для <script type="math/tex">y=0</script>:</p>

<script type="math/tex; mode=display">P(y = 0 \mid x) = P(y_i = 1 \mid x_i)^0 (1 - P(y_i = 1 \mid x_i))^{1 - 0} = 1 - P(y_i = 1 \mid x_i)</script>

<p>Правдоподобие для <script type="math/tex">m</script> наблюдений можно записать как:</p>

<script type="math/tex; mode=display">L = \prod_{i=1}^{m}P(y = y_i \mid x_i) = \prod_{i=1}^{m}P(y = 1 \mid x_i)^{y_i}(1 - P(y_i = 1 \mid x_i))^{1 - y_i}</script>

<script type="math/tex; mode=display">\log L = \sum_{i=1}^{m} y_i \times \log(P(y = 1 \mid x_i)) + (1 - y_i) \times \log(1 - P(y_i = 1 \mid x_i))</script>

<p>Введем обозначение:</p>

<script type="math/tex; mode=display">p_i = P(y = 1 \mid x_i)</script>

<p>Тогда функция правдоподобия может быть записана как:</p>

<script type="math/tex; mode=display">\log L = \sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))</script>

<p>Таким образом, мы можем записать нашу целевую функцию:</p>

<script type="math/tex; mode=display">J(\theta) = -\frac{1}{m}\sum_{i=1}^{m} y_i \times \log(p_i) + (1 - y_i) \times \log(1 - p_i))</script>

<p>Чтобы найти параметры <script type="math/tex">\theta</script> мы можем воспользоваться методом градиентного спуска. Вспомним правило обновления весов:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)</script>

<p>Где <script type="math/tex">\frac{\partial}{\partial \theta}J(\theta)</script>:</p>

<div class="admonition legend">
  <p class="first admonition-title"><strong>Замечание</strong></p>
  <p class="last">Вывод производных см. <a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/">тут</a>.</p>
</div>

<script type="math/tex; mode=display">\frac{\partial}{\partial \theta}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}</script>

<script type="math/tex; mode=display">h_{\theta}(x) = f(\theta^Tx) = \frac{1}{1 + e^{-\theta^{T}x}}</script>

<script type="math/tex; mode=display">f(z) = \frac{1}{1 + e ^{-z}}</script>

<p>В результате подстановки получим следующее правило для пересчета параметров <script type="math/tex">\theta</script>:</p>

<script type="math/tex; mode=display">\theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_i)-y_i)x_i</script>

<h3 id="пример-с-использованием-библиотеки-sklearn-1"><em>Пример с использованием библиотеки sklearn</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span> <span class="c"># 30</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> <span class="c"># 1</span>
</code></pre></div></div>

<p>Давайте рассмотрим другой пример, для которого сгенеируем набор данных из 10 тысяч наблюдений:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mi">2306</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mf">0.76866666666666672</span>
</code></pre></div></div>

<p>Значение <script type="math/tex">0.7687</script> ничего не говорит нам об ошибках <a href="https://ru.wikipedia.org/wiki/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B8_%D0%BF%D0%B5%D1%80%D0%B2%D0%BE%D0%B3%D0%BE_%D0%B8_%D0%B2%D1%82%D0%BE%D1%80%D0%BE%D0%B3%D0%BE_%D1%80%D0%BE%D0%B4%D0%B0">первого и второго рода</a>. Мы можем построить матрицу ошибок <script type="math/tex">C</script>, чтобы получить более полное представление.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">1129</span><span class="p">,</span>  <span class="mi">363</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">331</span><span class="p">,</span> <span class="mi">1177</span><span class="p">]])</span>
</code></pre></div></div>

<p>где:</p>
<ul>
  <li><script type="math/tex">C_{0,0} = 1129</script> – True Negatives (TN), истинное значение 0, предсказанное 0</li>
  <li><script type="math/tex">C_{1,0} = 331</script> – False Negatives (FN), истинное значение 1, предсказанное 0</li>
  <li><script type="math/tex">C_{0,1} = 363</script> – False Positives (FP), истинное значение 0, предсказанное 1</li>
  <li><script type="math/tex">C_{1,1} = 1177</script> – True Positives (TP), истинное значение 1, предсказанное 1</li>
</ul>

<p>И наконец мы можем построить ROC-кривую для визуальной оценки точности работы классификатора:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>

<span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkorange'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ROC curve (area = </span><span class="si">%0.2</span><span class="s">f)'</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'navy'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'ROC Curve'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="многоклассовая-классификация"><em>Многоклассовая классификация</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s">'ovr'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mi">42</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="mf">0.93333333333333335</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">12</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
       <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">14</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">y_predict_proba</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="c"># Compute ROC curve and ROC AUC for each class</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">fpr</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tpr</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">all_y_test_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
    <span class="n">all_y_predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
        <span class="n">y_test_i</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">yi</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">y_test</span><span class="p">]</span>
        <span class="n">all_y_test_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">all_y_test_i</span><span class="p">,</span> <span class="n">y_test_i</span><span class="p">])</span>
        <span class="n">all_y_predict_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">all_y_predict_proba</span><span class="p">,</span> <span class="n">y_predict_proba</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]])</span>
        <span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test_i</span><span class="p">,</span> <span class="n">y_predict_proba</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
        <span class="n">roc_auc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="c"># Compute micro-average ROC curve and ROC area</span>
    <span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">all_y_test_i</span><span class="p">,</span> <span class="n">all_y_predict_proba</span><span class="p">)</span>
    <span class="n">roc_auc</span><span class="p">[</span><span class="s">"average"</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">])</span>

    <span class="c"># Plot average ROC Curve</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="s">"average"</span><span class="p">],</span>
        <span class="n">label</span><span class="o">=</span><span class="s">'Average ROC curve (area = {0:0.2f})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">[</span><span class="s">"average"</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'deeppink'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="c"># Plot each individual ROC curve</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ROC curve of class {0} (area = {1:0.2f})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'k--'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Some extension of Receiver operating characteristic to multi-class'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="задание"><em>Задание</em></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<h3 id="кластеризация">Кластеризация</h3>

<p><strong>Кластерный анализ</strong> – это способ группировки многомерных объектов, основанный на представлении результатов отдельных наблюдений точками подходящего геометрического пространства с последующим выделением групп как «сгустков» этих точек (кластеров, таксонов). Задачей такого разделения на группы является ухватить естественную структуру данных и абстрагироваться от индивидуальных характеристик каждого объекта к более общим признакам, которые объединяют эти объекты в кластеры. Например, кластеризация документов по их содержимому или кластеризация покупателей по их потребительской корзине и т.д. Так как заранее не известно по каким признакам следует объединять объекты в кластеры, то кластерный анализ относят к методам <strong>обучения без учителя</strong> (unsupervised learning).</p>

<p>Одним из наиболее простых и распространенных алгоритмов кластеризации является алгоритм k-средних (k-means), в котором каждый кластер представлен его центром (центроидом).</p>

<ul>
  <li><script type="math/tex">k</script> означает число кластеров. Число кластеров не определяется автоматически и в каждом кластере может быть разное число объектов;</li>
  <li>k-средних использует двух шаговый эвристический подход к группированию похожих объектов: <strong>шаг присваивания</strong> и <strong>шаг обновления</strong>;</li>
  <li>схожесть (похожесть) объектов измеряется с помощью <strong>функции дистанции</strong>.</li>
</ul>

<h3 id="вычисление-дистанции-между-объектами"><em>Вычисление дистанции между объектами</em></h3>
<ul>
  <li>в методе k-средних значение каждого признака объекта воспринимается как координата в многомерном пространстве (например, если у нам известен только рост и вес человека, то мы имеем дело с двумерным объектом, где одна из координат это рост, а вторая - вес);</li>
  <li>схожесть объектов можно вычислить используя любую математическую функцию (метрику) схожести;</li>
  <li>обычно выбирается Евклидово расстояние:
 <script type="math/tex">dist(x,y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}</script></li>
</ul>

<h3 id="алгоритм-k-средних"><em>Алгоритм k-средних</em></h3>

<p>k-средних можно описать следующими 4-мя шагами:</p>
<ol>
  <li>Выбрать k объектов как начальные центроиды.</li>
  <li>Отнести остальные объекты к ближайшим центроидам.</li>
  <li>Произвести перерасчет центроидов.</li>
  <li>Повторять шаги 2 и 3 до тех пор, пока центроиды не перестанут «двигаться».</li>
</ol>

<p>Пример поиска трех кластеров, используя алгоритм K-средних представлен на рисунках ниже:</p>

<p><img src="/assets/images/11-ml/kmeans1.png" alt="" /><img src="/assets/images/11-ml/kmeans2.png" alt="" />
<img src="/assets/images/11-ml/kmeans3.png" alt="" /><img src="/assets/images/11-ml/kmeans4.png" alt="" />
<img src="/assets/images/11-ml/kmeans5.png" alt="" /><img src="/assets/images/11-ml/kmeans6.png" alt="" />
<img src="/assets/images/11-ml/kmeans7.png" alt="" /><img src="/assets/images/11-ml/kmeans8.png" alt="" /></p>

<h3 id="замечания-по-k-средних"><em>Замечания по k-средних</em></h3>
<ul>
  <li>k-средних эвристический алгоритм и не является детерминированным:
    <ul>
      <li>начальное положение центроидов оказывает существенное влияние на конечный результат;</li>
      <li>резуьтаты могут быть разными даже на одном и том же наборе данных.</li>
    </ul>
  </li>
  <li>так как используется расстояние для измерения схожести объектов, то все данные должны быть числовыми:
    <ul>
      <li>числовые данные должны быть приведены к единой шкале;</li>
      <li>могут возникнуть проблемы при кластеризации наборов данных состоящих из категориальных переменных.</li>
    </ul>
  </li>
  <li>иногда требуется знать предметную область и работать по методу проб и ошибок, чтобы получить нужные кластеры:
    <ul>
      <li>нужно заранее знать число кластеров <code class="highlighter-rouge">k</code>;</li>
      <li>может потребоваться оценка эксперта для ответа на вопрос <em>Являются ли полученные кластеры значимыми?</em></li>
    </ul>
  </li>
</ul>

<h3 id="как-выбирать-начальные-положения-центроидов"><em>Как выбирать начальные положения центроидов?</em></h3>
<ul>
  <li>можем случайным образом генерировать центры кластеров;</li>
  <li>или случайно выбрать k-объектов и назначить их центрами;</li>
  <li>но существуют и другие алгоритмы, которые позволяют увеличить точность и скорость сходимости k-средних, например:
    <blockquote>
      <p>Baswade, Nalwade. Selection of Initial Centroids for k-Means Algorithm</p>

      <ol>
        <li>From n objects calculate a point whose attribute values are average of n-objects attribute values. So first initial centroid is average on n-objects.</li>
        <li>Select next initial centroids from n-objects in such a way that the Euclidean distance of that object is maximum from other selected initial centroids.</li>
        <li>Repeat step 2 until we get k initial centroids. From these steps we will get initial centroids and with these initial centroids perform k-Means algorithm.</li>
      </ol>
    </blockquote>
  </li>
</ul>

<h3 id="подготовка-данных-к-кластеризации"><em>Подготовка данных к кластеризации</em></h3>

<p>Так как в методе k-средних значение каждого признака является координатой в многомерном пространстве, то все значения должны быть числовыми. Кроме того, хорошей практикой является приведение всех значений к единой шкале:</p>
<ul>
  <li>min-max нормализация:
 <script type="math/tex">x_{new} = \frac{x-min(x)}{max(x)-min(x)}</script></li>
  <li>z-score:
 <script type="math/tex">x_{new} = \frac{x - \mu}{\sigma} = \frac{x - mean(x)}{stddev(x)}</script></li>
  <li>введение порога:
 <script type="math/tex">x_{new} = 1 \mbox{ if } x \ge c \mbox{ else } 0</script></li>
</ul>

<p>Пример функций нормализации данных на языке R:</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create normalization function</span><span class="w">
</span><span class="n">normalize</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">((</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># create z-score standartization function (exists in R, @see ?scale)</span><span class="w">
</span><span class="n">scale</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sd</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="c1"># create threshold function</span><span class="w">
</span><span class="n">threshold</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">return</span><span class="w"> </span><span class="p">(</span><span class="n">ifelse</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Вашей задачей является написать класс <code class="highlighter-rouge">KMeans</code> со следующим интерфейсом:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KMeans</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>Для обучения модели будем использовать хорошо известный набор данных с лепестками цветов ириса:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.csv'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Name'</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">_clusters</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c"># Create a colormap</span>
<span class="n">colormap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s">'red'</span><span class="p">,</span> <span class="s">'lime'</span><span class="p">,</span> <span class="s">'black'</span><span class="p">])</span>

<span class="c"># Plot Sepal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">SepalLength</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">SepalWidth</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">clusters</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Sepal'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">PetalLength</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">PetalWidth</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colormap</span><span class="p">[</span><span class="n">clusters</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Petal'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="как-выбирать-число-кластеров"><em>Как выбирать число кластеров?</em></h3>

<p>Одной из проблем кластеризации является верное определение числа кластеров. Одним из используемых методов, для решения этой проблемы, является отображение зависимости изменения суммы квадратов ошибок от выбранного числа кластеров:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mydata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bd.rates</span><span class="w">
</span><span class="n">wss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">mydata</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">apply</span><span class="p">(</span><span class="n">mydata</span><span class="p">),</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">var</span><span class="p">))</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">15</span><span class="p">)</span><span class="w">
    </span><span class="n">wss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">kmeans</span><span class="p">(</span><span class="n">mydata</span><span class="p">,</span><span class="w"> </span><span class="n">centers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="o">$</span><span class="n">withinss</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">wss</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span><span class="w"> 
           </span><span class="n">xlab</span><span class="o">=</span><span class="s1">'Number of clusters'</span><span class="p">,</span><span class="w"> 
           </span><span class="n">ylab</span><span class="o">=</span><span class="s1">'Within groups sum of squares'</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>


  </div>

  
    <div class="post-comments" itemprop="comment">
      

    </div>
  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://dementiy.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
