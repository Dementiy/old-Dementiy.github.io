---
layout: post
title: Энтропия и кросс-энтропия
categories: python datascience
---

### Энтропия

[Академия Хана: Information Entropy](https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

Допустим у нас есть две механических машины, которые печатают сообщения из алфавита $${A, B, C, D}$$. Символы, печатаемые первой машиной, распределены равновероятно, то есть, появление каждого нового символа имеет вероятность $$0.25$$:

$$
P(A) = 0.25 \\
P(B) = 0.25 \\
P(C) = 0.25 \\
P(D) = 0.25
$$

Символы, печатаемые второй машиной, имеют иное распределение:

$$
P(A) = 0.5 \\
P(B) = 0.125 \\
P(C) = 0.125 \\
P(D) = 0.25
$$

<img src="/assets/images/notes-on-entropy/entropy1.jpg">

Вопрос «Какая из двух машин предоставляет нам больше информации?» или мы можем переформулировать вопрос: «Если вам необходимо предсказать следующий символ, то сколько вопросов, на которые можно ответить *да* или *нет*, потребуется задать?».

Давайте рассмотрим первую машину. Нашим первым вопросом может быть «Это символ A или B?». Так все символы появляются равновероятно, то с вероятностью $$0.5$$ это будет «A или B» и с вероятностью $$0.5$$ это будет «C или D». После того как мы получим ответ, нам останется задать еще один вопрос, например, «Это A?». И после того как мы получим ответ на наш последний вопрос, мы будем знать какой именно символ был следующим в последовательности, выдаваемой машиной один. Таким образом, нам достаточно двух вопросов, чтобы предсказать какой символ был сгенерирован машиной номер один.

<img src="/assets/images/notes-on-entropy/entropy2.jpg">

Что же касается второй машины, то мы конечно же можем задавть теже самые вопросы, но мы знаем, что у символов другое распределение, например, вероятность появления символа A равна $$0.5$$, соответственно он будет появляться в последовательности чаще остальных символов, поэтому более разумным было бы задать первый вопрос «Это A?». Если же ответ отрицательный, то следующий символ, который имеет более высокий шанс появиться в последовательности D, поэтому следующим вопросом может быть «Это D?». Если же ответ и на этот вопрос отрицательный, то мы задаем третий и последний вопрос, например, «Это B?».

<img src="/assets/images/notes-on-entropy/entropy3.jpg">

Сколько в среднем вопросов нам нужно задать, чтобы определить символ в последовательности генерируемой второй машиной?

Для этого мы можем воспользоваться формулой для вычисления математического ожидания дискретной случайной величины:

$$
\begin{aligned}
\textit{#вопросов} =& p_A \times 1 + p_B \times 2 + p_C \times 3 + p_D \times 2 \\
=& 0.5 \times 1 + 0.125 \times 3 + 0.125 \times 3 + 0.25 \times 2 \\
=& 1.75
\end{aligned}
$$

Аналогично посчитаем среднее число вопросов для первой машины:

$$
\begin{aligned}
\textit{#вопросов} =& p_A \times 2 + p_B \times 2 + p_C \times 2 + p_D \times 2 \\
=& 0.25 \times 2 + 0.25 \times 2 + 0.25 \times 2 + 0.25 \times 2 \\
=& 2.0
\end{aligned}
$$

Запишем в общем виде:

$$H = \sum_{i=1}^{n}p_i \times \textit{число_вопросов}_i$$

Где:

$$\textit{число_вопросов}_i = \log_2\left(\frac{1}{p_i}\right)$$

<img src="/assets/images/notes-on-entropy/entropy4.jpg">

Окончательно можем записать формулу для вычисления энтропии как:

$$H = \sum_{i=1}^{n}p_i \times \log_2\left(\frac{1}{p_i}\right) = -\sum_{i=1}^{n}p_i \times \log_2(p_i)$$


```python
from random import choices
from math import log2

distribution1 = {
    'A': 0.25,
    'B': 0.25,
    'C': 0.25,
    'D': 0.25
}

sequence1 = choices(
    population=list(distribution1.keys()),
    weights=distribution1.values(),
    k=100)

distribution2 = {
    'A': 0.5,
    'B': 0.125,
    'C': 0.125,
    'D': 0.25
}

sequence2 = choices(
    population=list(distribution2.keys()),
    weights=distribution2.values(),
    k=100)

>>> sum(map(lambda ch: log2(1 / distribution1[ch]), sequence1))
200.0
>>> sum(map(lambda ch: log2(1 / distribution2[ch]), sequence2))
170.0
```

### Кросс-энтропия